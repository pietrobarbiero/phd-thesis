\chapter{Categorical foundations of explainable AI and concept learning} \label{chapter:background}

% Story of chapter:
% \begin{itemize}
%     \item motivation: why do we need a categorical foundations of XAI? no existing sound theory of XAI, this has bad implications. Examples. Here we provide the first theory which will also set the scene for the next chapters. Why category theory? Because it is general and goes to the essence. We can really build the foundations.
%     \item elements of category theory: feedback monoidal categories, cartesian streams, signatures. 
%     \item syntax (learning agent, explainable learning agent) and semantics (translator functor, explanation, understanding) of explainable AI
%     % \item categorical taxonomy of explainable AI: 
%     \item Concept learning: why concepts? what is a concept (definition)? examples of concepts. Concept research focuses on: concept quality (completeness and purity, Chapter 3), concept representations (chapter 4), concept explanations (chapter 5), and interpretable concept models (chapter 6). This thesis makes progress in all these research directions.
% \end{itemize}

\textbf{Motivation}---Explainable AI (XAI) research aims to address the human need for accurate and trustworthy AI through the design of interpretable AI models and algorithms able to explain uninterpretable AI models~\cite{arrieta2020explainable}. 
Some of these methods are so effective that their impact now deeply affects other research disciplines such as medicine~\cite{jimenez2020drug}, physics~\cite{schmidt2009distilling,cranmer2019learning}, and even pure mathematics~\cite{davies2021advancing}. 

A considerable number of works attempted to describe key methods and notions in this fast-growing literature~\cite{adadi2018peeking,Das2020OpportunitiesAC,arrieta2020explainable,dovsilovic2018explainable,tjoa2020survey,gunning2019xai,hoffman2018metrics,palacio2021xai}. 
However, none of these works are grounded on a solid and unifying theory of explainability, but they rather rely on qualitative descriptions, preventing them from drawing truly universal conclusions. Current surveys acknowledge this problem and grumble that key fundamental notions of explainable AI still lack a formal definition, and that the field as a whole is missing a unifying and sound formalism~\cite{adadi2018peeking,palacio2021xai}: The very notion of ``\textit{explanation}'' represents a pivotal example as it still lacks a proper mathematical formalization. The followings represent an example of some of the best definitions currently available in literature:
{\small
\begin{displayquote}
\textit{``An explanation is an answer to a `}why?\textit{' question.''~\cite{miller2019explanation}}

\textit{``An explanation is additional meta information, generated by an external algorithm or by the machine learning model itself, to describe the feature importance or relevance of an input instance towards a particular output classification.''~\cite{Das2020OpportunitiesAC}}

\textit{``An explanation is the process of describing one or
more facts, such that it facilitates the understanding of aspects related to said facts (by a human
consumer).''}~\cite{palacio2021xai}.
\end{displayquote}
}

As the interest for XAI methods rises inside and outside academic environments, the need for a sound formalization and encompassing taxonomy of the field grows quickly, as an essential precondition to welcome a wider audience. Indeed, the absence of a mathematical formalization of key explainable AI notions may severely undermine this research field, as it could lead to ill-posed questions, induce re-discovery of the same ideas, and make it difficult for new researchers to approach the domain.

\textbf{Solution}---To fill this knowledge gap, in this chapter we introduce the elements of the first formal theory of explainable AI and concept learning aiming to:
\begin{itemize}
    \item formalize key XAI notions for the first time;
    \item set the scene and motivate the work of the next chapters.
\end{itemize}
The \textbf{key innovation} of this chapter is the use of categorical structures to formalize XAI notions and processes.
We use category theory as it provides a sound and abstract formalism to study general structures and systems of structures, avoiding contingent details and focusing on their very essence. For this reason, category theory represents now the standard formalism of many mathematical disciplines, including algebra~\cite{eilenberg1945general}, geometry~\cite{bredon2012sheaf}, logic~\cite{johnstone2014topos}, computer science~\cite{goguen1992institutions}, and more recently machine learning~\cite{cruttwell2022categorical,ong2022learnable}.
%This is why in this work we decided to describe XAI notions using this formal language.

In this chapter we set the scene for the next chapters defining the key notions and presenting the notation we will follow in the rest of this work in the language of category theory. We will first discuss the basic elements of category theory analyzing the main categorical structures we will use in the following chapters (Section~\ref{sec:pre}). We will then use such categories to formally define the syntax and the semantics of explainable AI agents and notions (Section~\ref{sec:framework} and~\ref{sec:syExp}). Among available semantics, we will focus on the human-friendly semantics based on concept learning (Section~\ref{sec:concept-learning}). Finally, we will present the main knowledge gaps in concept learning which motivate the next chapters (Section~\ref{sec:gaps-concept-learning}).

% CEMs is a fully supervised high-dimensional concept representation. This high-dimensional representation increases the capacity of CEMs at concept level. The increased model capacity allows to encode more information in each concept beyond the probability of a concept being active/inactive, including contextual nuances which CEMs can use to have a deeper understanding of each concept and to solve tasks more efficiently.

% We will first present CEM's architecture~\ref{sec:cem} and then we will demonstrate how CEMs fill the key CBMs knowledge gaps we discussed with a set of experiments of increasing complexity~\ref{sec:cem}.

% For this reason, in this chapter we introduce the elements of the first formal theory of explainable AI and concept learning. This way we set the scene for the next chapters defining the key notions and presenting the notation we will follow in the rest of this work in the language of category theory. 

% Specifically, we use category theory as it provides a sound and abstract formalism to study general structures and systems of structures, focusing on their essence. 
% For this reason, many mathematical disciplines are affected by category theory, including algebra~\cite{eilenberg1945general}, geometry~\cite{bredon2012sheaf}, logic~\cite{johnstone2014topos}, and more recently machine learning~\cite{cruttwell2022categorical,ong2022learnable}.


% \section{Notation and basic definitions}
\section{Elements of category theory}
\label{sec:pre}
To make this work self-contained, this section introduces the minimal set of definitions that we will later need to formalize XAI systems i.e., feedback monoidal categories and the category of signatures. In particular, we will use feedback monoidal categories as a syntax to model structures sharing some of the key properties of AI systems, being able to: observe inputs, provide outputs, and receive feedback dynamically. Cartesian streams provide a semantics for these models. We will use the category of signatures to model the structure of ``explanations''.
% Further details can be found in the apendix.

\subsection{Monoidal categories}
The process interpretation of monoidal categories~\cite{Coecke2017,fritz2020} sees morphisms in monoidal categories as modelling processes with multiple inputs and multiple outputs.
Monoidal categories also provide an intuitive syntax for them through string diagrams~\cite{joyal1991geometry}.
The coherence theorem for monoidal categories~\cite{maclane78} ensures that string diagrams are a sound and complete syntax for them and thus all coherence equations for monoidal categories correspond to continuous deformations of string diagrams. One of the main advantages of string diagrams is that they make reasoning with equational theories more intuitive.

We recall the definitions of category and monoidal category.
Categories provide a syntax for processes that can be composed \emph{sequentially}.
\begin{definition}[\citet{eilenberg1945general}]
    A \emph{category} \(\cat{C}\) is given by a class of \emph{objects} $\obj{\cat{C}}$ and, for every two objects \(X,Y \in \obj{\cat{C}}\), a set of \emph{morphisms} $\hom(X,Y)$ with input type \(X\) and output type \(Y\). A morphism \(f \in \hom(X,Y)\) is written \(f \colon X \to Y\).
    For all morphisms \(f \colon X \to Y\) and morphisms \(g \colon Y \to Z\) there is a \emph{composite} morphisms \(f \dcomp g \colon X \to Z\).
    For each object \(X \in \obj{\cat{C}}\) there is an \emph{identity} morphism \(\id{X} \in \hom(X,X)\).
    Composition needs to be associative, i.e. there is no ambiguity in writing \(f \dcomp g \dcomp h\), and unital, i.e. \(f \dcomp \id{Y} = f = \id{X} \dcomp f\).
    % (\Cref{fig:category-string-diagrams}, bottom).
\end{definition}
%\begin{figure}[h!]
%    \centering
    $\sequentialFig{}$ \qquad \(\identityXFig\)\\[4pt]
    \compositionUnitalFig{}
    % \caption{String diagrams for the composition of two morphisms (top, left), the identity morphism (top, right), and the unitality condition (bottom).}\label{fig:category-string-diagrams}
%\end{figure}
A mapping between two categories $\cat{C}_1$ and $\cat{C}_2$ that preserves compositions and identities is called a \emph{functor} and maps objects and morphisms of $\cat{C}_1$ into objects and morphisms of $\cat{C}_2$.

% A \emph{category} consists of a class of \emph{objects} $\mathcal{O}$ and a class of \emph{morphisms} $\mathcal{H}$.
% \mike{maybe the $\mathcal{O}$ and $\mathcal{H}$ be in the end of the name because now it cut the flow of reading. Something like class of objects $\mathcal{O}$ .....}. 
% Each morphism $f\in\mathcal{H}$ has a domain object $X\in\mathcal{O}$ and a codomain object $Y\in\mathcal{O}$, written \(f \colon X \to Y\). Morphisms with a common interface can be composed and composition satisfies few axioms (see~\citet{maclane78} for more details). 
\begin{example}
$\Set$ is a category whose objects are sets (e.g., $X = \{\text{tree}, \text{sky}\}$ or $Y = \{\text{green}, \text{blue}\}$) and whose morphisms are functions between sets (e.g., $f: X \rightarrow Y$ such that $\text{blue} = f(\text{sky})$ and $\text{green} = f(\text{tree})$). 
\end{example}

Monoidal categories~\cite{maclane78} are categories endowed with extra structure, a monoidal product and a monoidal unit, that allows morphisms to be composed \emph{in parallel}.
The monoidal product is a functor \(\tensor \colon \cat{C} \times \cat{C} \to \cat{C}\) that associates to two processes, \(f_1 \colon X_1 \to Y_1\) and \(f_2 \colon X_2 \to Y_2\), their parallel composition \(f_1 \tensor f_2 \colon X_1 \tensor X_2 \to Y_1 \tensor Y_2\) (\Cref{fig:string-diagrams-monoidal-cat}, right).
The monoidal unit is an object \(\monoidalunit \in \obj{\cat{C}}\).
A monoidal category is \emph{symmetric} if there is a morphism \(\swap{X,Y} \colon X \tensor Y \to Y \tensor X\), for any two objects \(X\) and \(Y\), called the \emph{symmetry}.
%\begin{figure}[h!]
%    \centering
    $\morphismManyWiresFig{} \qquad \parallelFig{} \qquad \swapXYFig{X}{Y}$
    % \caption{A morphism with multiple inputs and outputs (left), the parallel composition of two morphisms (center), and the symmetry (right) in a monoidal category.}
%    \label{fig:string-diagrams-monoidal-cat}
%\end{figure}
A symmetric monoidal structure on a category is required to satisfy some coherence conditions \cite{maclane78} (see~\Cref{app:mon-cat} for more details), which ensure that string diagrams are a sound and complete syntax for symmetric monoidal categories \cite{joyal1991geometry}. 
Like functors are mappings between categories that preserve their structure, \emph{symmetric monoidal functors} are mappings between symmetric monoidal categories that preserve the structure and axioms of symmetric monoidal categories.


Some symmetric monoidal categories have the additional property of allowing resources and processes to be \emph{copied} and \emph{discarded}.
These are called \emph{Cartesian categories}.
A monoidal category \(\cat{C}\) is Cartesian whenever there are two morphisms, the copy \(\cp_X \colon X \to X \times X\) and the discard \(\discard_X \colon X \to 1\) (\Cref{fig:copy-discard}), that commute with all morphisms in \(\cat{C}\)~\cite{fox76} (see~\Cref{app:cat} for details).
When a monoidal category is Cartesian, it is customary to indicate with \(\times\) the monoidal product given by the Cartesian structure, and with \(1\) the corresponding monoidal unit. 
%\begin{figure}[h!]
    \[\copyXFig{} \qquad \qquad \discardXFig{}\]
    % \caption{The copy and discard morphisms in a Cartesian category.}
%    \label{fig:copy-discard}
%\end{figure}


%\paragraph{Monoidal category}
%A category with an associative unital tensor product, subject to certain coherence conditions, is said a \emph{Monoidal Category}~\cite{maclane78}. More specifically, a monoidal category has an additional structure defined by an operation $\times$ on both objects, like $X \times Y$ and with $I$ denoting the identity object, and morphisms, such that if $f \colon X_1 \rightarrow Y_1$ and $g \colon  X_2\rightarrow Y_2$ then 
%$f \times g \colon X_1 \times X_2 \rightarrow Y_1 \times Y_2$ 
%(satisfying the axioms in Appendix~\ref{app:cat}).
%In a monoidal category, a morphism \(f \colon X \to Y\) represents a process with input type \(X\) and output type \(Y\) (e.g. with possibly multiple inputs $X_1,X_2,X_3$ and outputs $Y_1,Y_2$). String diagrams can \emph{formally} describe a morphism as a box with input and output wires~\cite{joyal1991geometry}: 
%\[
%\morphismFig{} \hspace{1cm}
%\morphismManyWiresFig{}\]
% In monoidal categories, processes can have multiple inputs and multiple outputs:
% For example a morphism \(h \colon X_1 \tensor X_2 \tensor X_3 \to Y_1 \tensor Y_2\) is depicted as a box with multiple input and output wires: 
%The structure of a monoidal category allows two kinds of composition of processes: sequential and parallel.
%Given two morphisms \(f \colon X \to Y\) and \(g \colon Y \to Z\), they can be composed sequentially to obtain \(f\dcomp g \colon X \to Z\) which represents the process of applying \(f\) and then applying \(g\) to the output of \(f\) i.e.,
% This is depicted by connecting the output wire of \(f\) with the input wire of \(g\)
%$\sequentialFig{}$. Given \(f \colon X \to Y\) and \(f' \colon X' \to Y'\), they can also be composed in parallel to obtain \(f \times f' \colon X \times X' \to Y \times Y'\) i.e.,
% This is depicted by writing \(f\) on top of \(f'\).
%$\parallelFig{}$. 
%For every object \(X\), there exists an identity morphism \(\id{X} \colon X \to X\), i.e. such that \(f \dcomp \id{Y} = f = \id{X} \dcomp f\), given \(f \colon X \to Y\).
%This is depicted by a wire with type \(X\): \(\identityXFig\).
%The equation is intuitive in string diagrams:

% An example is \Cref{fig:composition-unital}.
% Another example is the middle-four interchange law \((f \tensor f') \dcomp (g \tensor g') = (f \dcomp g) \tensor (f' \dcomp g')\).
% These two expressions have one representation in terms of string diagrams.
% \[\interchangeLawFig{}\]
%The coherence theorem~\cite{maclane78} for monoidal categories ensures that string diagrams are a sound and complete syntax for morphisms in monoidal categories.



\subsection{Feedback monoidal categories and Cartesian streams}
With symmetric monoidal categories we can model AI systems that observe inputs and produce outputs.
Most AI learning algorithms, however, rely on \emph{feedback} to adjust their learning parameters: the learning phase for AI models is often dynamic, observing inputs, producing outputs, and getting feedback over and over again. 
Feedback monoidal categories provide a structure, on top of the structure of symmetric monoidal categories, to model this dynamic behaviour.

\begin{definition}[\cite{katis02,di2021canonical}]
A feedback monoidal category is a symmetric monoidal category \(\cat{C}\) endowed with an endofunctor $F:\cat{C} \rightarrow \cat{C}$, and an operation \(\fbk[S] \colon \hom (X \times F(S), Y \times S) \to \hom (X,Y)\) for all objects \(X,Y,S\) in \(\cat{C}\), which satisfies a set of axioms (see \Cref{app:feedback-cat}).
\end{definition}
% \elena{write diagrams! and fix itemize}\\
% \elena{feedback functors}\\
% \elena{free feedback monoidal categories}\\
%In addition, the learning phase for AI models is often dynamic, observing inputs, producing outputs, and getting feedback over and over again. To model this dynamic behavior, we can use the category of Cartesian streams~\cite{uustalu05,katsumata19}. 
%To define this category we need a Cartesian category i.e., a special monoidal structure allowing for resources to be copied \(\cp_X \colon X \to X \times X\) and discarded \(\discard_X \colon X \to 1\)~\cite{fox76}:
% We indicate with \(\times\) the monoidal product given by the Cartesian structure, and with \(1\) the corresponding monoidal unit. 
% for every object \(X\), with two morphisms \(\cp_X \colon X \to X \times X\) (\textit{copy}) and \(\discard_X \colon X \to 1\) (\textit{discard}):
%\[\copyXFig{} \qquad \text{and} \qquad \discardXFig{}\]
Feedback monoidal functors are mappings between feedback monoidal categories that preserve the structure and axioms of feedback monoidal categories.
Feedback monoidal categories are the \emph{syntax} for processes with feedback loops.
When the monoidal structure of a feedback monoidal category is cartesian, we call it feedback cartesian category.
Their \emph{semantics} can be given by monoidal streams~\cite{monoidalStreams}.
In cartesian categories, these have an explicit description.
We refer to them as cartesian streams, but they have appeared in the literature multiple times under the name of ``stateful morphism sequences''~\cite{katsumata19} and ``causal stream functions''~\cite{uustalu05}.

\begin{definition}[\citet{uustalu05}]
A \emph{cartesian stream} \(\stream{f} \colon \stream{X} \to \stream{Y}\), with \(\stream{X} = (X_0, X_1, \dots)\) and \(\stream{Y} = (Y_0, Y_1, \dots)\), is a family of functions \(f_n \colon X_n \times \cdots \times X_0 \to Y_n\) indexed by natural numbers.
Cartesian streams form a category \(\Stream{\Set}\).
For the details of this construction see \Cref{app:streams}.
\end{definition}
% \begin{proposition}[\citet*{monoidalStreams}]
% Streams form a feedback monoidal category.
% \end{proposition}

The main purpose for using Cartesian streams is their capability of capturing an entire flow of a training process while using the framework of a category.
A morphism in the category of cartesian streams encodes a process that receives an input \(X_n\) and produces an output \(Y_n\) at each time step \(n\).
%In this setting \(\stream{X} = (X_0, X_1, \dots)\) and \(\stream{Y} = (Y_0, Y_1, \dots)\) have the meaning to encode the flow of the input and output sets of the process $\stream{f}$ during the training.

% \todo{To check}

\begin{proposition}[\citet*{monoidalStreams}]
Cartesian streams form a feedback monoidal category denoted by \(\Stream{\Set}\).
\end{proposition}

As a result, we can use the category of Cartesian streams as semantics for structures sharing some of the key properties of AI systems, i.e. being able to: observe inputs, provide outputs, and receive feedback dynamically.


\subsection{Free monoidal categories and syntax}
%\elena{mention functorial semantics somewhere~\cite{lawvere1963functorial}, cite something for free constructions, for adjoints~\cite{kan1958adjoint}}
A syntax is a way of reasoning abstractly about structures without the need to know the details of any given structure.
In the same way a traditional syntax is defined by a set of symbols and some rules to combine them, free symmetric monoidal categories and free feedback monoidal categories are defined by a set of generators for objects and for morphisms.
The rules to combine them are given by the structure and axioms of symmetric monoidal categories and feedback monoidal categories, respectively.

When reasoning with a syntax, we want to ensure that the reasoning carried out still holds in the semantics.
This is done by symmetric monoidal functors, in the case of symmetric monoidal categorie, and feedback functors, in the case of feedback monoidal categories.
In fact, by definition of free symmetric monoidal (resp. feedback monoidal) category, once we fix the semantics of the generators, there exist a unique symmetric monoidal (feedback monoidal) functor to the semantics category.

We will employ free feedback monoidal categories as syntax for learning agents, and take semantics in the feedback monoidal category \(\Stream{\Set}\) of cartesian streams.

% \stefano{Proposta def free category: A \emph{free category} generated by a set of morphisms $\mathcal{G}$ is a category whose objects are source or target objects of generators in $\mathcal{G}$ and whose morphisms are the tuples of composable functions of $\mathcal{G}$. Intuitively, given a set of axioms for the composition, a free category generated by a set of morphisms is the most "general" category containing the generators and satisfying the chosen axioms.}



% \usepackage{graphicx}
\begin{table}[!t]
\centering
\caption{Reference for notation. List of main operations, objects, morphisms, categories, and functors.}
\label{tab:notation}
\resizebox{\columnwidth}{!}{%\small
\begin{tabularx}{\columnwidth}{cX}
% {\small
\toprule
\multicolumn{1}{c}{\textbf{\textsc{Symbol}}} & \multicolumn{1}{l}{\textbf{\textsc{Description}}} \\ \midrule
% \multicolumn{2}{c}{\textbf{Operations}}  \\
\multicolumn{2}{c}{\textbf{Objects}}  \\
$X$ & \textit{Input}: the input type of a model. \\
$Y$ & \textit{Output}: the output type of a model. \\
$P$ & \textit{Parameter}: the type of a model state. \\
$E$ & \textit{Explanation}: the output type of an explainer. \\
\\
\multicolumn{2}{c}{\textbf{Morphisms}}  \\
$\id{Z}$ & \textit{Identity}: the identity operation on $Z$. \\
$\hat{g}$ & \textit{Model}: given an input $X$ and parameter $P$, returns an output $Y$. \\
$\nabla_Y$ & \textit{Optimizer}: given a reference $Y$, a model output $Y$ and a parameter $P$, returns the updated parameter $P$. \\ 
$\hat{f}$ & \textit{Explainer}: given an input $X'$ and a parameter $P$, returns an output $Y'$ and an explanation $E$. \\ 
% $\tau$ & Translator: given an explanation $\mathcal{E}$, returns an output prediction $Y$. \\
\\
\multicolumn{2}{c}{\textbf{Categories}}  \\
\(\Stream{\Set}\) & \textit{Cartesian streams}: feedback monoidal category of Cartesian streams on $\Set$. \\
\(\mathsf{Learn}\) & \textit{Category of learners}: free feedback monoidal category generated by the objects $X$, $Y$, $P$, and the morphisms $g$ and $\nabla$.\\
\(\mathsf{XLearn}\) & \textit{Category of explainable learners}: free feedback monoidal category generated by the objects $X$, $Y$, $P$, $E$ and the morphisms $\eta$, $\nabla_Y$, and $\nabla_E$. \\
\(\mathsf{Sign}\) & \textit{Category of signatures}: category generated by the object $\Sigma$ and the morphism $\phi$. \\
\\
\multicolumn{2}{c}{\textbf{Functors \& Operators}}  \\
$\fbk[S]$ & \textit{Feedback}: the operation which brings an output back to the input. \\
\(Sen\) & \textit{Sentence}: functor from the category of signatures $\mathsf{Sign}$ to $\Sigma$-sentences over $\Set$. \\
$T$ & \textit{Interpreter}: functor from the category of learners \(\mathsf{Learn}\) to Cartesian streams $\Stream{\Set}$ over $\Set$. \\
\bottomrule
\end{tabularx}%
}
\end{table}
% \mike{This Table is a good idea to introduce it in the begin of the terminologies so the reader have it as a reference when he/she is confused. as there are a lot of terminologies. }
% \section{Categories of Learning Agents: Explainable and Concept-based Learners}

\subsection{Category of signatures}
In order to model objects of type ``explanation'', we will use the category of signatures~\cite{goguen1992institutions}.
% is  categorical theory of institutions and abstract formal concepts~\cite{goguen2005concept} study abstract syntactic structures. 
In institution theory~\cite{goguen1992institutions}, a signature $\Sigma$ constitutes the ``syntax'' of a formal language which serves as ``context'' or ``interpretant'' in the sense of classical logic~\cite{goguen2005concept}. Simple examples of signatures are given by First-Order Logic (FOL) theories and equational signatures.
% \begin{definition}[Signature~\citep{goguen1992institutions}]
% A \emph{signature} $\Sigma = (S_f, S_r, \text{ar})$ is a collection of:
% \begin{itemize}
%     \item a set of function symbols $S_f$
%     \item a set of relation symbols (or predicates) $S_r$
%     \item a morphism $\text{ar}: S_f \cup S_r \rightarrow \mathbb{N}$, which assigns a natural number called \textit{arity} to every function or relation symbol \alberto{The notation ``ar'' sucks, it is kinda incoherent with the rest. But I guess we are forced to use it because of previous papers, right?}
% \end{itemize}
% \end{definition}
% \stefano{I would not use the word morphism in this definition. Morphisms in mathemathics and in the paper have a specific meaning. I would use "function"}
Signatures form a category $\mathsf{Sign}$ whose objects are signatures and whose morphisms $\phi: \Sigma \rightarrow \Sigma'$ are interpretations between signatures corresponding to a ``change of notation''~\cite{goguen1992institutions}.
From this abstract vocabulary, institution theory defines abstract statements as sentences obtained from a vocabulary $\Sigma$~\cite{goguen1992institutions}.
\begin{definition}[$\Sigma$-sentence~\cite{goguen2005concept}]
There is a functor $Sen: \mathsf{Sign} \rightarrow \Set$ mapping each signature $\Sigma$ to the set of statements $Sen(\Sigma)$.
A $\Sigma$-\emph{sentence} is an element of \(Sen(\Sigma)\).
\end{definition}

\begin{example}\label{ex:sigma-sentence}
Let $\Sigma$ be a signature of propositional logic 
with $\{x_{\textit{flies}},$ $x_{\textit{animal}},$ $x_{\textit{plane}},$ $x_{\textit{dark\_color}},$ $\ldots\} = \textit{VAR}$, being $\textit{VAR}$ an infinite set of propositional variables and the standard connectives of Boolean logic, i.e. $\neg,\wedge,\vee,\rightarrow$. Then $x_{\textit{plane}} \wedge x_{\textit{flies}}$ is a $\Sigma$-sentence.
% Let $f$ be an explainer aiming at predicting an output in $\mc{Y}=\{x_{\textit{plane}}\}$ given an input in $\mc{X}\subseteq\textit{VAR}$.
\end{example}

% and each signature morphism $\sigma: \Sigma \rightarrow \Sigma'$ to the sentence translation map $Sen(\sigma): Sen(\Sigma) \rightarrow Sen(\Sigma')$.


%%%% moved in sec 3.4
% \citet{tarski1944semantic} and~\citet{goguen1992institutions} proved how the semantics of ``truth'' is invariant under change of signature. This means that we can safely use signature morphisms to %change ``notation'' 
% switch from one ``notation'' to another, inducing consistent syntactic changes in a $\Sigma$-sentence without impacting the ``meaning'' or the ``conclusion'' of the sentence~\cite{goguen1992institutions}. As a result, signature morphisms can translate a certain explanation between different signatures. 
%%%%%%%



% \begin{example}
% % A simple example of a $\Sigma$-sentence 
% A valid sentence in natural language is ``All men are mortal. Socrates is a man.
% Therefore, Socrates is mortal.''. Using a functor from natural language to first-order logic \elena{is there a functor from natural language to FOL? is natural language even a syntax in the mathematical sense?}, we can write the syllogism as ``$\forall x\ \text{man}(x) \rightarrow \text{mortal}(x)$, $\text{man}(Socrates) \Longrightarrow \text{mortal}(Socrates)$'', without changing its validity.
% \end{example}



\section{Syntax and semantics of explainable AI}
\label{sec:framework}
Here we formalize XAI structures and semantics using feedback monoidal categories and the category signatures $\mathsf{Sign}$. 
To this end, we first formalize the notions of ``learning agent'' (Section~\ref{sec:learning-agent}) and ``explainable learning agent'' (Section~\ref{sec:x-learning-agent}) as morphisms in free feedback monoidal categories generated by a \emph{model}, an \emph{optimizer}, and an \emph{explainer}.  Then we describe a functor translating these abstract notions into concrete instances in the feedback monoidal category of \(\Stream{\Set}\) (Section~\ref{sec:interpretation}). 
%%% OLD
Finally, we formalize the notion of ``explanation'' as a $\Sigma$-theory and of ``understanding'' as a signature morphism in (Section~\ref{sec:syExp}).
% To this end, we first describe the
% abstract entity we call “learning agent” which represents an
% abstraction for any human or artificial learner (Section 3.1).
% Secondly, from learning agents we derive another abstract
% entity we call “explainable learning agent” which represents
% an abstraction for special instances of learning agents which
% generate objects called “explanations” (Section 3.2). Finally,
% we describe a functor which translates these abstract entities
% into the concrete category of Set (Section 3.4). For guid-
% ance, Table 1 contains the main reference to the notation we
% use in this and following sections.
% \stefano{I would remove the distinction between concrete and non-concrete categories since it doesn't make sense in category theory}

% \todo{Overview of aims}:
% \begin{itemize}
%     \item aim 1: define learning agents
%     \item aim 2: define explainers
%     \item aim 3: interaction between learning agents / explainers
%     \item aim 4: define data for explanations
% \end{itemize}



\subsection{Syntax of learning agents}
\label{sec:learning-agent}
% \paragraph{What is learning?}
% We provide the formal definition of a learning agent from an abstract description of its features. In general, learning is a dynamic process which depends on a set of parameters $P$, like e.g. the weights of a neural network. More specifically, 
Generalizing~\citet{cruttwell2022categorical} to non-gradient-based systems, learning involves the following processes:
\begin{itemize}
    \item \textbf{Observing} a pair of objects $X$ and $Y$. In AI the objects $X$ and $Y$ represent input and output data of models.
    \item \textbf{Predicting} objects of type $Y$ from objects of type $X$, given a parameters of type $P$.
    \item \textbf{Updating} parameters $P$ according to a loss function. 
    % \alberto{But we are not updating the states $P$, we are updating the states of the parameters $P$, right?}
\end{itemize}
% \mike{Please kindly make more clear the $P$ what is represent because it is confusing. 'we ofter refer to the 'states P' as "parameters" ' or at P as "parameters". If it is the second, then in predicting terminology the 'parameters P' is redundancy}

Using this informal description as guidance,  we describe an abstract learning agent
% \(s\) \todo{do we use this symbol later?} 
as a morphism in the free feedback monoidal category generated by two morphisms: a model \(\hat{g} \colon X \times P \to Y\) and an optimizer \(\nabla_Y \colon Y \times Y \times P \to P\). The model $g$ produces an output of type \(Y\) given an input of type \(X\) and a parameter of type \(P\), while $\nabla_Y$ updates the parameters of the model \(P\) given a reference of type \(Y\), the predicted output of type \(Y\), and the parameters \(P\).
In order to specify the syntax of abstract learning agents we define the free category $\mathsf{Learn}$ of abstract models and optimizers.
% \todo{here we generate a free category because we want to generate the syntax of our systems}
\begin{definition}[Abstract model and optimizer]
The category $\mathsf{Learn}$ is the free feedback cartesian category generated by three objects, the input type \(X\), the output type \(Y\) and the parameter type \(P\), and by two morphisms, the model \(\hat{g} \colon X \times P \to Y\) and the optimizer \(\nabla_Y \colon Y \times Y \times P \to P\).
\[\learnModel \quad \learnOptimizer\]
% \todo{FIGURE: Add a few space between the letters}
\end{definition}
\begin{remark}
The output of the model and the reference may contain different elements, but they do have the same type, which is why we use the same type-symbol $Y$ to represent both objects. The same argument applies in the following whenever we have conceptually different inputs/outputs but denoting objects of the same type.
\end{remark}
Having defined the free category for abstract models and optimizers, we formalize an abstract learning agent as a morphism in $\mathsf{Learn}$.
% \stefano{do we need a $Y'$ for the optimizer?}
% \mike{the optimizer has to Y. If i understand correctly the one is the out put of the g morphism and the other is the reference? ground truth of supervised learning?}
% \mike{the next string diagram is more clear than the above one. Can you name different the Y reference/groundtruth so the reader knows that is not the output of g?}
\begin{definition}[Abstract learning agent]
An abstract learning agent is the morphism in $\mathsf{Learn}$ given by the composition $\fbk[P]\left((\id{Y}\times\id{X}\times\nu_P);(\id{Y}\times \hat{g} \times \id{P});\nabla_Y\right):$
% $(\id{Y} \times g) \dcomp \nabla)$:
% $\fbk[P]((\id{Y} \times g) \dcomp \nabla)$
\[\learnCat\]
% \fg{I think the full dot should be before the split with $g$ and I'll represent somehow the $\fbk[P]$?} 
\end{definition}
% \todo{do we need this proposition?}
% \begin{proposition}
% % \elena{Consider whether we want to do this, maybe with the Para construction?}
% Learning agents form a feedback monoidal category. 
% \end{proposition}


\subsection{Syntax of explaining agents}
\label{sec:x-learning-agent}
% \paragraph{What is explainable learning?}
% After defining what an abstract learning agent is, w
% Here describe what makes a learning agent explainable. As for learning agents, we derive a formal definition of an explainable learning agent from an abstract description of its features. \todo{add running example} 
Compared to learning, learning to explain involves the following processes:
\begin{itemize}
    \item \textbf{Observing} a pair of objects $X$ and $Y$.
    \item \textbf{Predicting} objects of type $Y$ from the observed $X$ given a set of parameters $P$.
    \item \textbf{Predicting} explanation objects of type $E$
    % with signature $\Sigma$, 
    from the observed $X$ given the parameters $P$.
     \item \textbf{Updating} the parameters $P$ according to a loss function over the predicted objects $Y$ or $E$.
% \mike{It is a bit tricky the 'updating' here. There are explainable learning agents that they did not need any loss minimization and training process (local XAI like attribution (shap,LRP,lime) and sensitivity methods (GradCam etc). except if by explainable learning agents you mean something different that i did not understand.??? }
% \fg{An explaining learning agent should assume a training procedure, while a simple explainer can just be used to predict an explanation. We can make an example referring then to the next section where there are all the differet instantiations.}
\end{itemize}
\begin{remark}
    Not all XAI methods have or need to update parameters. In our formalism, we describe the updating process of these ``static'' systems with an identity morphism.
\end{remark}
From this informal description we conclude that learning to explain extends learning processes manipulating an extra object, called ``explanation''. To define the category of explainable learning agents we need: (i) a new morphism $\hat{f}: X \times P \rightarrow Y \times E$, called \textit{explainer}, that provides both predictions $Y$ and explanations $E$,
% $\nabla_{Y'}: Y' \times Y' \times P' \rightarrow P'$ 
and (ii) a new morphism $\nabla_E: E \times E \times P \rightarrow P$ to optimize the agent parameters through its explanations $E$.
% \mike{again same comments as in learning agent. E and E I will kindly suggest to be different like $E_r$ or $E_{GT}$ or something that shows reference or ground truth.}
 \begin{definition}[Abstract explainer and optimizer]
     The category $\mathsf{XLearn}$ is the free feedback cartesian category generated by four objects, the input type $X$, the output types $Y$ and $E$, the parameter type $P$, and by three morphisms, the explainer $\hat{f}: X \times P \rightarrow Y\times E$, 
     % , the translator $\tau: \mathcal{E} \rightarrow \mathcal{Y}$, 
     and the optimizer $\nabla_{Y}: Y \times Y \times P \rightarrow P$: 
     % and $\nabla_E: E \times E \times P \rightarrow P$:
\[\learnExplainer \quad \learnOptimizerX \]%\quad \learnOptimizerExplanations \]
 \end{definition}
% Notice how the composition of an explainer and a translator $\eta \circ \tau$ yields a model of type $g$, and thus the category $\mathsf{XLearn}$ is a subcategory of $\mathsf{Learn}$ \todo{please check this!!}.
% \stefano{As free monoidal categories should be the other way round since the set of generators of XLearn is bigger than Learn but I'm not sure}
Through these new morphisms we formalize an abstract learning agent manipulating explanations.
% \todo{sometimes the explanation is a by-product}
% \stefano{no def of XLearn}
% \mike{again E, E , Y, Y some readers will think is the same}
\begin{definition}[Abstract explainable learning agent]
An abstract explainable learning agent is the morphism in $\mathsf{XLearn}$ given by the composition:
\[ \fbk[P]\left((\id{Y\times X}\times\nu_{P});(\id{Y}\times \hat{f} \times \id{P}); (\id{Y\times Y}\times\swap{E,P}) ;(\nabla_{Y}\times \discard_E)\right)\]
% $\fbk[P]((\id{Y} \times \eta ) \dcomp \nabla)$:
\[\xLearnCat\]
% \elena{change the diagram to send E as output to the top right, same for Y below}
% or % by the composition 
% ${\scriptstyle \fbk[P]\left((\id{E\times X}\times\nu_{P});(\id{E}\times \eta \times \id{P}); (\id{E}\times \swap{Y,E\times P});(\nabla_{E}\times \discard_Y)\right):}$
% % $\fbk[P]((\id{E} \times \eta ) \dcomp \nabla_E)$:
% \[\xLearnCatExplanations\]
% \[\fbk[P]((\id{E} \times \eta ) \dcomp \nabla_E).\]
% \[\xLearnCat\]
\end{definition}
\begin{remark}
    Our formalism allows for two different optimization processes to update the parameters of explainable learning agents. The first option optimizes explanations indirectly by applying a standard optimizer $\nabla_{Y}$ over the predictions of the explainer and reference labels of type $Y$. The second option instead optimizes explanations directly by applying the optimizer $\nabla_E$ over the explanations of the explainer and reference explanations of type $E$.
\end{remark}
% \todo{add definition of agreement?}
% \begin{proposition}
% % \elena{Consider whether we want to do this, maybe with the Para construction?}
% Explainable learning agents form a feedback monoidal category. 
% \end{proposition}
% Notice how in our formalism ``explaining'' is not necessarily a dynamic process as learning. In general, explaining is a static process if the explainer is detached from the translator. Thus, in our framework the primary function of the translator is to make the explaining process dynamic by mapping explanations $\mathcal{E}$ into outputs $Y$ which an optimizer $\nabla$ can process to update the parameters $P$.

% \begin{remark}
%     To be more general, we defined the free categories of $\mathsf{Learn}$ and $\mathsf{XLearn}$ by using different object types $X$, $Y$, $P$. 
%     However, we can always define a unified free category with an explainer working on the same input/output type of a learning agent (as often happens in the practice). To generate this category we can use the objects $X,Y,P,E$ and the morpisms $g:X\times P\rightarrow Y$, $\eta:X\times P\rightarrow Y\times E$, $\nabla_Y:Y\times Y\times P\rightarrow P$, $\nabla_E:E\times E\times P\rightarrow P$.
% \end{remark}




\subsection{Semantics of AI agents}
\label{sec:interpretation}
So far we described key notions of XAI in an abstract way, i.e. in terms of free feedback monoidal categories. However, we need these notions to be ``concrete'' before using them in typical AI settings. To this end, we specify the semantics of abstract agents via a functor from the free category $\mathsf{Learn}$ to the concrete category of Cartesian streams over \(\Set\), i.e. \(\Stream{\Set}\). With this functor we can model the specific types of inputs, outputs, parameters, explanations, and signature we need in a specific context. We refer to such a functor as ``translator'' and define it as follows.
\begin{definition}[Translator]

\begin{itemize}
\item[]
    \item A functor \textbf{$\mathrm{T}_A$} from $\mathsf{Learn}$ to \(\Stream{\Set}\) is said an agent translator.
    \item Given a signature $\Sigma$, a functor $\mathrm{T}_{\Sigma}$ from $\mathsf{XLearn}$ to \(\Stream{\Set}\), where $\mathrm{T}_{\Sigma}(E)\subseteq Sen(\Sigma)$ is said an explainer translator.
\end{itemize}
\end{definition}

Through these functors we can specify concrete semantics and describe typical AI functions and objects. For instance, we can map abstract objects and morphisms from $\mathsf{Learn}$ and $\mathsf{XLearn}$ to concrete streams of sets and functions. To simplify the notation we will use the following shortcuts: $\mathcal{X} = \mathrm{T}_A(X)$, $\mathcal{X'} = \mathrm{T}_{\Sigma}(X)$, $\mathcal{Y} = \mathrm{T}_A(Y)$, $\mathcal{Y'} = \mathrm{T}_{\Sigma}(Y)$, $\mathcal{P} = \mathrm{T}_A(P)$, $\mathcal{P'} = \mathrm{T}_{\Sigma}(P)$, and $\mathcal{E} = \mathrm{T}_{\Sigma}(E)$. In AI these objects often take values in vector spaces e.g., $\mathcal{X} \subseteq \mathbb{R}^n$, $\mathcal{P} \subseteq \mathbb{R}^p$, $\mathcal{Y} \subseteq \mathbb{R}^t$. In addition, for clarity we will denote $g=T_A(\hat{g})$, $f=T_{\Sigma}(\hat{f})$ and $\hat{\nabla}=T(\nabla)$.
Translator functors allow us to model different types of real-world learners, including AI agents.
\begin{definition}[Concrete learning agent]
Given a translator $\mathrm{T}_A$ between $\mathsf{Learn}$ and $\mathsf{Stream_{Set}}$, we call concrete learning agent, or simply an AI agent, the image $\mathrm{T}_A(\alpha)$ of the abstract learning agent, where
${\displaystyle \alpha = \fbk[P]\left((\id{Y}\times\id{X}\times\nu_P);(\id{Y}\times g \times \id{P});\nabla_Y\right).}$
% \todo{not sure how to frame this one...}
% \textit{Human agent} A human agent is a concrete learning agent whose objects are in \(\Set\) and whose morphisms are functions in the human brain.
\end{definition}
% Using the translator functor $\mathrm{T}$, we can also describe some of the most common learning paradigms in AI such as supervised and unsupervised learning.
% % \elena{All the following definitions should be given in terms of the shape of the semantics functor}\todo{what do you mean?}
% \begin{example}[Learning Paradigms]
% \textit{Supervised Learning. } Supervised Learning is a concrete learning process whose morphisms are functions in \(\Set\), whose objects are in \(\Set\), and whose optimizer updates the parameters $P$ such that the output $Y$ is close to the input labels $Y$.
% \textit{Unsupervised Learning. } Unsupervised learning is a concrete learning process whose morphisms are functions in \(\Set\), whose objects are in \(\Set\), and whose input $Y$ is empty.
% \end{example}
% \stefano{If we mention morphisms we have to mention their category (Learn). I would not use concrete}
% \begin{example}[Gradient-based Learning Agent]
% Using $\mathrm{T}_A$, we can also model the categorical definition of gradient-based learning proposed by~\citet{cruttwell2022categorical} as a special case:
% % , which includes some of the most studied objects in explainable AI i.e., neural networks~\cite{lecun2015deep}.
% A gradient-based learning agent is an AI agent whose optimizer generates parameters updates using the gradient of a differentiable loss function over the parameters.
% % of type $\mathcal{L}(Y,g(X))$ w.r.t. the current parameter states $P_t$ i.e., $P_{t+1} = P_{t} - \alpha \frac{\partial \mathcal{L}(Y,g(X))}{\partial P_t}$.
% \end{example}
We can use this formalism to provide a more precise view of the dynamic process of learning.
% \begin{definition}[Learning]
% Learning is the process of a learning agent which keeps updating its parameters until it reaches a stationary state. 
% where  $\frac{\partial P}{\partial t} \rightarrow 0$ 
% i.e. $\nabla_Y \approx (\epsilon_Y \times \epsilon_Y \times\id{P}$).
% \fg{to check}
% \todo{Improve the description by cartesian streams? example of learning agent?
% exists n such taht.... then convergence}
% \todo{add example showing how we can update the paramters}
% \todo{e se lo cambiassimo direttamente in esempio di una rete neurale? e gli si aggancia direttamente come esce il learning sullo stream}
In fact we can describe learning as the process of a concrete learning agent which keeps updating its parameters until it eventually reaches a stationary state. Given a learning agent $L$ this process is represented by the image of $L$ through the translator functor $T_A$. A learning process is convergent if there exist $k$ such that the cartesian stream $T_A(L)$ has $g_{n+1} \approx g_n| X_n \times \cdots \times X_0$ and $X_{n+1} = X_n$ for $n > k$.
% \todo{definizione di convergenza del learning: c'è convergenza quando $\nabla(y,y,p)=p$ per ogni $y\in \mc{Y}, p\in \mc{P}$
% Non credo ci sia convergenza in codesto caso. Per dare la definizione di convergenza dobbiamo usare gli streams perchè è li che è codificato il training}
% \end{definition}


\begin{definition}[Concrete explainable agent]
Given a translator $\mathrm{T}_{\Sigma}$ between $\mathsf{XLearn}$ and $\mathsf{Stream_{Set}}$, we call concrete explainable agent, or simply an AI agent, the image $\mathrm{T}_{\Sigma}(\alpha)$ of the abstract explainable agent, where:
${\scriptstyle \alpha=\fbk[P]\left((\id{Y\times X}\times\nu_{P});(\id{Y}\times f \times \id{P}); (\id{Y\times Y}\times\swap{E,P}) ;(\nabla_{Y}\times \discard_E)\right)}$. 
% or ${\scriptstyle \alpha=\fbk[P]\left((\id{E\times X}\times\nu_{P});(\id{E}\times \eta \times \id{P}); (\id{E}\times \swap{Y,E\times P});(\nabla_{E}\times \discard_Y)\right)}$
% \[
% \begin{array}{l}
% \fbk[P]((\id{Y\times X}\times\nu_{P});(\id{Y}\times \eta \times \id{P});\\
% ;(\id{Y\times Y}\times\swap{E,P}) ;(\nabla_{Y}\times \discard_E))
% \end{array}
% \]
% \todo{not sure how to frame this one...}
% \textit{Human agent} A human agent is a concrete learning agent whose objects are in \(\Set\) and whose morphisms are functions in the human brain.
\end{definition}



%\begin{example}[Gradient-based Learning Agent]
    %This definition can be seen a special case of an instantiation of $\mathsf{Learn}$ through a translator $\mathrm{T}_A$, so as the category $\mathsf{ParaLens}$ they refer to is a special case of $\mathsf{Stream}_{\mathsf{Set}}$ category \todo{provide reference for a theorem, or change the sentence} describing only one step of the training. For what concern string diagrams representation, we notice that our diagram represent a generalization of the one used in~\citet{cruttwell2022categorical}, where both the optimizer and the loss function are included in the $\nabla_Y$ morphism. 
    % (see Fig. \ref{fig:gblavsala}).
    % \begin{figure}
    %     \centering
    %     \includegraphics[width=0.5\textwidth]{figs/string diagrams.png}
    %     \caption{Gradient-based learning agents proposed by \cite{cruttwell2022categorical} are a special case of a concrete learning agents.}
    %     \label{fig:gblavsala}
    % \end{figure}

%\end{example}



% \begin{itemize}
%     \item we need now to go from abstract categories to concrete categories
%     \item first we define an interpreter as a functor mapping free cat in concrete cat
%     \item then we can start discussing: learning paradigms (supervised vs unsupervised), human vs AI learner, gradient-based learner, classifiers\&regressors, etc
% \end{itemize}

% \todo{The following are examples}

% \elena{All the following definitions should be given in terms of the shape of the semantics functor}

% \begin{definition}[Adversarial Learning]
% Adversarial learning is a learning process where the optimizer updates the parameters $P$ such that the predictions $B$ are far from the input supervisions $B$.
% \end{definition}

% Figure \ref{fig:learning_agent} formally describes the category of learning agents \(\Learn{\cat{Stream}}\) by means of its string diagrams. Figure \ref{fig:learning_agent_composition} formally describes the composition of learning agents in series and in parallel.
% The composition of learning agents is still a learning agent. The composition of models is still a model. A model can be a learning agent by itself (with its own internal model and optimizer).
% \begin{figure}[!h]
%     \centering
%     \includegraphics[width=0.2\textwidth]{figs/cat4xai_3-simple block.png}
%     \includegraphics[width=0.2\textwidth]{figs/cat4xai_3-learning.png}
%     \caption{Learning agent: $s = g \circ \nabla$.}
%     \label{fig:learning_agent}
% \end{figure}
% \begin{figure}[!h]
%     \centering
%     \includegraphics[width=0.2\textwidth]{figs/cat4xai_3-composition.png}
%     \includegraphics[width=0.2\textwidth]{figs/cat4xai_3-comp parallel.png}\\
%     \includegraphics[width=0.2\textwidth]{figs/cat4xai_3-learning_comp.png}
%     \caption{Compositions of agents: in series and in parallel.}
%     \label{fig:learning_agent_composition}
% \end{figure}

% \todo{The following are examples}

% \elena{I am not sure what \(\Learn{\Stream{}}\) should be}

% \begin{example}[Learning Classifier]
% A learning classifier is a concrete learning agent where supervisions and predictions are objects $A,B \in [0,1]^k$.
% \end{example}


% From objects to concepts 

% \section{``Explain'' the Explainable AI Literature}


% Add:
% \begin{itemize}
%     \item agreement (concordant explanations): $\frac{\partial P}{\partial t} \rightarrow 0$ with $\nabla_E$
% \end{itemize}
% \mike{Please consider to include a small introduction to explain the difference of an explainable agent and explainable method, so the reader can know what will read in the section 3 and 4. Right now the manuscript is a bit confusing as you start immidiatly with literature review and terminology. As i was reading the manuscript I was expected to read about XAI methods and generalized terminology. In section 3 that you speak about the XAI agents, i was thinking about the XAI methods which you explain in 4 and this confused me a lot. Just include a small paragraph of 3 sentences that explain the XAI framework with the XAI agency, the method and the communication for the explanation before start the terminology (a scheme will be even better!)}


\section{Explanations and understanding}
\label{sec:syExp}
\subsection{What is an explanation?}
So far we just considered an ``explanation'' as a special object generated by an explainer morphism $\hat{f}$, depending on its input $X$ and/or parameters $P$. 
% According to the introduced definition, an explainable learning agent distinguishes from a learning agent as it also generates a special object $E$, called ``explanation''. 
We are now interested in analyzing the properties of this special object.
As previously discussed, the XAI research community agrees in considering explanations as ``answers to \textit{why?} questions''~\cite{miller2019explanation}. 
% In our categorical framework, an explanation is an output of the explainer $\eta$, depending on its input $X'$ and/or parameters $P'$. %~\cite{Das2020OpportunitiesAC}.
% A typical explanation can thus describe a causal relationships between events. Humans typically disambiguate events in ``causes'' and ``effects'' such that whenever the ``causes'' occur, then the ``effects'' must follow~\cite{todo}. 
% In our formalism all objects  the learning agent parameters $P$, the data $X$ are the ``causes'' while the agent predictions $Y$ are the ``effects''.
Here we generalize this idea providing the first formal definition of the term ``explanation'', which embodies the very essence and purpose of explainable AI.
% \todo{change notation for single explanation and explanation object type}
\begin{definition}[Explanation]
    Given a $\Sigma$ signature and a concrete explainer $f=T_{\Sigma}(\hat{f}):\mc{X}\times \mc{P}\rightarrow \mc{Y}\times \mc{E}$,
    % over the input $\mc{X}$ and parameters $\mc{P}$, 
    an explanation $\mc{E} = T_{\Sigma}(E)$ in a signature $\Sigma$ is a set of $\Sigma$-sentences (i.e. a $\Sigma$-theory). 
\end{definition}

% \begin{example}
%         Let assume  $\Sigma$ to be a signature of Propositional Logic 
%     with $\{x\} = \textit{VAR}$, being $\textit{VAR}$ the set of propositional variables, a set of relations $R = \{Files(x), Animal(x), Plane(x), \dots\}$ and the standard connectives of Boolean Logic, i.e. $\neg,\wedge,\vee,\rightarrow$. For instance, $\hat{\eta}$ could be an explainer aiming at predicting an output in $\mc{Y}=\{\textit{plane}\}$ given an input in $\mc{X}$.    
%     Then an explanation could consist in a $\Sigma$ sentence, like 
%     % $\{\textit{Plane}(x) \rightarrow p_{\textit{flies}}$, $p_{\textit{flies}} \wedge \neg p_{\textit{animal}}$, $
%     $\varepsilon = Files(x) \wedge\neg Animal(x)\rightarrow Plane(x)$.
% \end{example}





% \begin{remark}
Our definition of explanation generalizes and formalizes the best definitions currently available in literature such as the ones we presented at the beginning of this chapter. In fact, existing definitions informally represent special forms of explanations. For example, according to~\citet{Das2020OpportunitiesAC} and~\citet{palacio2021xai} an explanation provides additional meta information to describe facts related to the explainer, including the feature relevance of an input. This represent the simplest form of explanation and corresponds to a pure description of the most relevant inputs. Seminal XAI methods typically provide this form of descriptions by showing the most relevant input attributes for the prediction of a given sample, as it happens in saliency maps~\cite{simonyan2013deep}, Concept Activation Vectors~\cite{kim2018interpretability}, and SHapley Additive exPlanations~\cite{lundberg2017unified}. The following example illustrates this form of explanation.
\begin{example}
\label{ex:logrel}
Let $\Sigma'$ be a vocabulary extending the one in Example~\ref{ex:sigma-sentence} with two additional symbols $R=\{\textit{relevant}, \textit{irrelevant}\}$. We consider as sentences in this language, expressions of kind $(x_1:r_1,\ldots,x_n:r_n)$, where $x_i\in \textit{VAR}$ and $r_i\in R$ for $n\in\mathbb{N}$, $i=1,\ldots,n$. Let $f$ be an explainer aiming at predicting an output in $\mc{Y}=\{x_{\textit{plane}}\}$ given an input in $\mc{X}\subseteq\textit{VAR}$. Then an explanation describing the most relevant inputs is a $\Sigma'$-sentence such as $\varepsilon'=(x_{\textit{flies}}:\textit{relevant},x_{\textit{animal}}:\textit{relevant},x_{\textit{dark\_color}}:\textit{irrelevant})$.
\end{example}
A more advanced form of explanation describes specific combinations of attributes leading to specific predictions. This form of explanation is common in rule-based systems such as decision trees~\cite{breiman1984classification} and Generalized Additive Models~\cite{hastie2017generalized}. Explanations of this form may represent an answer to a ``why?'' question such as to why a specific input instance leads to a specific output~\cite{miller2019explanation,Das2020OpportunitiesAC}, as we illustrate in the following example.
\begin{example}
\label{ex:logsig}
    Let $\Sigma$ be the vocabulary in Example~\ref{ex:sigma-sentence}. Let $f$ be an explainer aiming at predicting an output in $\mc{Y}=\{x_{\textit{plane}}\}$ given an input in $\mc{X}\subseteq\textit{VAR}$. Then the $\Sigma$-sentence $\varepsilon = x_{\textit{flies}}\wedge\neg x_{\textit{animal}}\rightarrow x_{\textit{plane}}$ explains why the input is classified as type ``plane''.
\end{example}

% \end{remark}
% As a simple example of a $\Sigma$-sentence in natural language is ``All men are mortal. Socrates is a man.
% Therefore, Socrates is mortal.''. 
% Using a functor from natural language to first-order logic, we can write the syllogism as ``$\forall x\ \text{man}(x) \rightarrow \text{mortal}(x)$, $\text{man}(Socrates) \Longrightarrow \text{mortal}(Socrates)$'', without changing its validity.
% \todo{PIANO: esempio della rilevanza+tarski e co + def comunicazione tra explainer? + esempio conversione di spiegazioni.. .e quindi understanding!}
% The simplest form of explanation is a pure description of the most relevant inputs~\cite{Das2020OpportunitiesAC}. Seminal XAI methods typically provide this form of descriptions by showing the most relevant input attributes for the prediction of a given sample, as it happens in saliency maps~\cite{simonyan2013deep}, Concept Activation Vectors~\cite{kim2018interpretability}, and SHapley Additive exPlanations~\cite{lundberg2017unified} . A more advanced form of explanation describes specific combinations of attributes leading to specific predictions. This form of explanation is common in rule-based systems such as decision trees~\cite{breiman1984classification} and Generalized Additive Models~\cite{hastie2017generalized}. 
% Finally, following existing surveys~\cite{Das2020OpportunitiesAC}, we can further distinguish the semantics of explanations on whether they hold for a single sample (as in LIME~\cite{ribeiro2016should}) or for a more general group of samples (as in decision trees~\cite{breiman1984classification}).

% \todo{esempio da togliere, non è una signature per come la intende Gougen}

\begin{remark}
\citet{tarski1944semantic} and~\citet{goguen1992institutions} proved how the semantics of ``truth'' is invariant under change of signature. This means that we can safely use signature morphisms to %change ``notation'' 
switch from one ``notation'' to another, inducing consistent syntactic changes in a $\Sigma$-sentence without impacting the ``meaning'' or the ``conclusion'' of the sentence~\cite{goguen1992institutions}. As a result, signature morphisms can translate a certain explanation between different signatures.
\end{remark}
While signature morphisms do not change the meaning of an explanation, they may have a great impact on human observers as we discuss in the next section.


% \paragraph{Semantics of Explanations}
% While data semantics supplies the raw material for $\Sigma$-sentences, the form of a sentence determines the . 


%%%%%%work on it for the rebuttal
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% PIE IN CASO TAGLIA TUTTO DA QUA ALLA FINE DELLA SEZ
% \todo{TOCHECK!! and in case remove (I would say also the understanding part)}
% On the other hand, in our framework an explanation is tightly related to the specific inputs and outputs of an explainable agent, then it is fundamental for comparing explanations in different signatures, to define how two explainers  can ``communicate" between them.
% \begin{definition}[Explainers' communication]
% Given two explainer $\hat{\eta}_1:\mc{X}_1\times\mc{P}_1\rightarrow \mc{Y}_1\times\mc{E}_1$ and $\hat{\eta}_2:\mc{X}_2\times\mc{P}_2\rightarrow \mc{Y}_2\times\mc{E}_2$ with signatures $\Sigma_1$ and $\Sigma_2$ respectively, we say that $\hat{\eta}_1$ can communicate with $\hat{\eta}_2$ if it exists a function $\Phi:\mc{X}_1\times\mc{P}_1\times\mc{Y}_1\times\mc{E}_1\rightarrow \mc{E}_2$, such that $\Phi_{|\mc{E}_1}$ is a morphism between $Sen(\Sigma_1)$ and $Sen(\Sigma_2)$.
% \end{definition}
% \begin{example}
%     As an example of communication between explainable agents, and following Example \ref{ex:logsig} and \ref{ex:logrel}, we can consider $\Phi(x,p,y,\varepsilon)=(x_i:\textit{relevant},\ldots,x_j:\textit{irrelevant})$ if $x_i$s occur in $\varepsilon$ whereas $x_j$s do not.
% \end{example}
% \todo{To make more correct or delete it}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%






\subsection{What is understanding?}
Tightly connected to explanation morphisms, ``understanding'' is another key notion in explainable AI which currently lacks a mathematical formalization. In the context of explainable AI, we are often interested in a specific type of understanding which~\citet{pritchard2009knowledge} refers to as \textit{understanding-why}. This form of understanding is often called \textit{explanatory understanding} and is ascribed in sentences that take the form ``I understand why Z'', where Z is an explanation (for example, ``I understand why the bread burnt as I left the oven on''). 
% Notice how this informal description is tightly connected with the informal description of an explanation as ``an answer to a \textit{why?} question''~\cite{miller2019explanation}. 
Using this intuition, we can formally define understanding as follows.
% \todo{agreement vs understanding}
% \todo{change to interpretation}
%%%% TO FIX DEPENDING ON THE PREVIOUS PART
\begin{definition}[Understanding]
    An explainable learning agent providing explanations in a signature $\Sigma'$ can understand the explanation $\mathcal{E}$ in the signature $\Sigma$ if and only if it exists at least one signature morphism $\phi: \Sigma \rightarrow \Sigma'$.
    % from the signature $\Sigma$ of the explanation and the signature $\Sigma'$ of the agent.
\end{definition}

\begin{remark}
    Notice that the existence of this morphism is not always guaranteed. This means that in some cases human observers may not be able to understand certain AI explanations. This happens even among human beings talking in two different (natural) languages. In other situations, a partial morphism may exist allowing a form of partial understanding. This happens for example in translating natural languages to formal languages.
    % Nevertheless, when two agents share the same ``language'', to allow for a kind of understanding to exist. \todo{add discussion on partial understanding e.g., from natural language to formal language}
\end{remark}

For this reason, choosing a good signature is key and often more important for human understanding than developing state-of-the-art explainers. In fact, signatures based on ambiguous syntax (e.g., natural language) may significantly degrade human understanding as bits and pieces of explanations might get lost in the change of notation. Conversely, signatures of formal languages (e.g., propositional logic) are robust under translation in other languages, including informal languages, providing stronger guarantees for human observers. The second aspect of a good signature is the choice of the symbols providing the raw material for compound explanations. The next section illustrates how the choice of symbols plays a crucial role for human understanding.

% Another interesting research line in AI consists in studying multi-agents learning environments, with possibly the human in the loop.  
% According to our proposed formalization, for instance we can define  the notion of ``agreement'' between two explainers by co-optimizing their explanations until convergence, or evaluating their communicated explanations.
% % \begin{definition}[Agreement]
% %     Given two explainable learning agents $\hat{\eta}$ and $\hat{\eta}'$ providing explanations in a signature $\Sigma$ and $\Sigma'$ if and only if exists a communication function $\Phi$ between them, such that $\nabla_{E'}$ converges if provided $\mc{E}'$ and $\Phi(\mc{E})$ as inputs.
% % \end{definition}
% % \todo{a questo punto 1 explainer capisce l'altro quando tramite il morfismo glielo posso attaccare al $\nabla_E$ e mi da convergenza..?}

% % \subsection{Semantics of Explanations}
% \label{sec:xai-semantics}
% While the taxonomy of XAI models mostly depends on how explainer and model functions are combined, the semantics of explanations mostly depends 
% % on the content of the data objects and the signature used to form sentences. 
% % The semantics of the explanation depends 
% on two factors: the semantics of the data and the form of a valid $\Sigma$-sentence.
% % While category theory formalizes explainable AI structures, the semantics of explanations mostly depends on the content of the objects transformed by the morphisms. From a categorical perspective, all the objects handled by AI models are ``formal concepts'' in \(\Set\), as we discussed in Section~\ref{sec:framework}. Given this general structure, we can still differentiate the semantics of data with regards to the semantics of explanations, as follows.

% % \paragraph{Semantics of data} 



\section{Concepts: semantics for human understanding}
\label{sec:concept-learning}

\subsection{Data semantics and human understanding}
The semantics of data forms the raw material for the semantics of explanations and plays a crucial role for human understanding. We describe the semantics of data in terms of the set of attributes used to characterize each sample and on the set of values each attribute can take. We usually refer to data objects as feature and label matrixes, corresponding to input objects $\mathcal{X}$ and target $\mathcal{Y}$ respectively. The semantics of a feature matrix varies depending on the attributes which typically represent pixels in images~\cite{kulkarni2022explainable}, relations in graphs~\cite{li2022survey}, words in natural language~\cite{danilevsky2020survey}, or semantically-meaningful variables (such as ``temperature'', ``shape'', or ``color'') in tabular data~\cite{di2022explainable}. Notice how different data types do not change the architecture of an explainable AI system. However, choosing a specific data type can lead to significantly different levels of human understanding. In fact, human understanding does not depend directly on the structure of the explainable AI system, but rather on the existence and completeness of a proper signature morphism from the explanation to the human observer. For example, humans lean towards explanations whose semantics is based on meaningful, human-understandable notions (such as ``temperature'', ``shape'', or ``color''), rather than explanations whose semantics is based on pixels.
% , as pointed out in seminal works in concept learning~\cite{kim2018interpretability}. 
In fact several works show how humans do not reason in terms of low-level attributes like pixels, but rather in terms of high-level ideas~\cite{goguen2005concept,ghorbani2019interpretation}. Thus explanations based on such semantics might significantly improve human understanding~\cite{ghorbani2019interpretation}.
This observations have roots in cognitive sciences (e.g., Representational Theory of the Mind). According to these theories ``\emph{concepts are the basic building blocks of human thoughts}''~\citep{margolis2007ontology}: following simple rules the human mind can combine finite stocks of basic concepts over and over again to create increasingly complex representations~\citep{margolis2007ontology}. For instance, the mind can combine the basic concepts ``roof'' and ``walls'' to generate the concept ``house''. 

\subsection{What is a concept?}
The relationship between data semantics and human understanding motivated~\citet{kim2018interpretability} to open a research line in concept learning in AI to increase human understanding. The objective of this field is to increase human trust by making AI use ``the same building blocks of human thought'' as opposed to other XAI approaches~\citep{kim2018interpretability}. Informally, we can define a concept as a human-understandable property shared by a set of objects. For instance ``roof'' is a property shared by all objects of type ``house''. Likewise, we can say that all objects of type ``house'' share the property of having a ``roof''.
% In particular concept-based XAI has the following aims:
% \begin{itemize}
%     \item discover concepts from trained models to explain their decisions in ``human'' terms (post-hoc explainability)
%     \item train models to learn specific concepts and compose them to solve tasks (self-explainability)
% \end{itemize} 
% Compared to other XAI approaches, concept-based XAI has solid foundations in cognitive sciences and a precise formalization in universal algebra.
Following~\citet{ganter1997formal} and~\citet{goguen2005concept}, we formalize the notion of ``concept'' as follows.
\begin{definition}[(Formal) Context~\citep{ganter1997formal}]
A formal context $\mc{K} := (\mc{A},\mc{B},\mc{I})$ consists of a set of objects $\mc{A}$, a set of attributes $\mc{B}$, and a set of relations $\mc{I}$ between $\mc{A}$ and $\mc{B}$.
\end{definition}
In order to express that an object $a \in \mc{A}$ is in relation with an attribute $b \in \mc{B}$, we write $(a, b) \in \mc{I}$ and read it as ``the object $a$ has the attribute $b$". For a set $\mc{A}' \subseteq \mc{A}$ of objects we can define the set of attributes common to the objects in $\mc{A}$: $\mc{A}^* := \{b \in \mc{B} \ | \ \forall a \in \mc{A}', \ (a,b) \in \mc{I} \}$. Similarly, we can define the the set of objects having all attributes in $\mc{B}' \subseteq \mc{B}$: $\mc{B}^* := \{b \in \mc{B} \ | \ \forall b \in \mc{B}', \ (a,b) \in \mc{I} \}$.
\begin{definition}[(Formal) Concept~\citep{ganter1997formal}]
A concept of the context $\mc{K} := (\mc{A},\mc{B},\mc{I})$ is a pair $(\mc{A}',\mc{B}')$ such that $\mc{A}' \subseteq \mc{A}$, $\mc{B}' \subseteq \mc{B}$, $\mc{A}^* = \mc{B}$, and $\mc{B}^* = \mc{A}$.
\end{definition}
\citet{ganter1997formal} refers to $\mc{A}'$ as the \textit{extent} and to $\mc{B}'$ as the \textit{intent} of the concept $(\mc{A}',\mc{B}')$. In AI, we often represent (formal) contexts using matrices where the rows are headed by sample identifiers, the columns are headed by attribute names, and the value of a cell represents the binary relation between a sample and an attribute. In these settings we often discriminate among different type of contexts depending on their use. Following common practice, we call ``feature matrix'' the context corresponding to the input type of an AI agent and we represent this context with the set $\mc{X} \subseteq \mathbb{R}^{d}$. We call ``label matrix'' the context corresponding to the output type of an AI agent and we represent this context with the set $\mc{Y} \subseteq \mathbb{R}^{l}$. 
% \begin{itemize}
%     \item $A \subseteq \{0,1\}^{n \times n}$ commonly known as adjacency matrix
%     \item $Y \subseteq \mathbb{R}^{n \times l}$ commonly known as the matrix of ``ground-truth labels''
%     % \item $C \subseteq \mathbb{R}^{n \times k}$ commonly known as the matrix of ``ground truth attribute labels''
% \end{itemize}
\begin{remark}
The same concept can have different representations depending on its intent $\mc{B}'$. In particular the intent plays a key role in assigning specific semantics to the context, thus affecting the semantics of explanations and human understanding, as illustrated in the following example.
\end{remark}
% For instance, two images representing the concept ``house'' need to have the exact same pixels representing the ``house''. This is because the context of ``pixels'' is not translation or rotation invariant. 
\begin{example}
Consider the concept ``house'' for a single sample described using different attributes i.e., pixels (image), node neighbors (graph), or labels (table):
\begin{figure}[H]
    \centering
%    \includegraphics{image_concept.pdf}
%    \includegraphics{graph_concept.pdf}
%    \includegraphics{label_concept.pdf}
    % \caption{input matrix: segment of image (subset of pixels), adjacency matrix: motif of graph (subset of nodes and edges), label matrix: a rectangle with a non-zero elements of the table (subset of rows and columns)}
    \label{fig:my_label}
\end{figure}
Notice how the form of the explanations is considerably different and some are less intuitive than others.
\end{example}
In particular,~\citet{kim2018interpretability} observe that when the intent is less ``structured'' (e.g., attributes represent pixels of an image) explanations can be quite difficult to grasp for humans. This is why~\citet{kim2018interpretability} propose to increase human understanding by providing explanations based on contexts where the individual attribute names are semantically meaningful and human-understandable (as it often happens in tabular data). For this reason, when the intent of the feature matrix is not human-understandable,~\citet{kim2018interpretability} propose to transform the original intent into a more semantically meaningful set of attributes where concepts and explanations are more intuitive for human observers. 
\begin{remark}
    For brevity and simplicity,~\citet{kim2018interpretability} refers to human-understandable attributes as ``high-level concepts'' or simply as ``concepts''. From now on we will follow this convention and we refer to ``human-understandable attributes'' as ``concepts'' highlighting the distinction with ``formal concepts'' when appropriate.
\end{remark}
The following definition will simplify our description in the next chapters.
\begin{definition}[High-level concept~\cite{kim2018interpretability}]
    A high-level concept (or simply ``concept'') is a human-understandable and semantically meaningful attribute name.
\end{definition}
In the next section we describe the general structure of AI agents providing explanations in human-understandable semantics based on high-level concepts.

\subsection{Concept-based models}
Concept-based models are explainable AI agents generating predictions using human-understandable concepts as input~\cite{kim2018interpretability,chen2020concept,koh2020concept}. Through the input concepts, concept-based models aim to increase human trust by allowing their users to trace back predictions directly to human-understandable concepts thus making the whole decision process of the AI agent more transparent~\cite{rudin2019stop,shen2022trust}. For instance, a concept-based model can make the prediction $\mc{Y}=\{x_{bird}\}$ using the concepts $x_{flies}$ and $x_{animal}$ allowing a human observer to verify that the set of concepts used to make the prediction matches their experience.

Concept-based models $f: \mathcal{C} \times \mc{P} \rightarrow \mathcal{Y}$ learn a map from a set of semantically meaningful concepts $\mc{C}$ to a set of tasks $\mathcal{Y}$~\cite{yeh2020completeness}. This way humans can interpret this mapping by tracing back predictions to the most relevant concepts~\cite{ghorbani2019interpretation}. When the features of the input space are hard for humans to reason about (such as pixel intensities), we may still apply concept-based models on the output of a ``concept-encoder'' i.e., a mapping $g: \mc{X} \times \mc{P}' \rightarrow \mc{C}$ from the input space $\mc{X}$ to the concept space $\mc{C}$~\cite{ghorbani2019towards,koh2020concept}. 
Using our categorical constructions we can formally describe a concept-based model as follows.
\begin{definition}[Concept encoder]
    A concept encoder is an AI agent $g: \mathcal{X} \times \mathcal{P}' \rightarrow \mathcal{C}$ where the output object $\mc{C}$ represents a set of concepts\footnote{Concepts in the sense of~\citet{kim2018interpretability}.}.
\end{definition}
\begin{definition}[Concept-based model]
    Given a concept encoder $g: \mathcal{X} \times \mathcal{P}' \rightarrow \mathcal{C}$, 
    a concept-based model is a XAI model where the explainer $f: \mathcal{C} \times \mathcal{P} \rightarrow  \mathcal{Y}\times \mathcal{E}$ takes as input object the set of concepts $\mathcal{C}$ generated by the concept encoder: 
    % \todo{fix tikz}
\[ \xaiCBM \]
\end{definition}
% % \stefano{Is XAI system defined?}
% % \paragraph{Agreement between explainers}
% This example demonstrates how category theory allows to reason about structures of structures more easily freeing XAI researchers from contingent details. 
Training a concept-based model may require a dataset 
% composed of tuples in $\mathcal{D} = (\mathcal{X} \times \mathcal{C} \times \mathcal{Y})\subseteq(X \times C \times Y)$,
where each sample consists of input features $\mathbf{x}\in \mathcal{X} \subseteq \mathbb{R}^n$ (e.g., an image's pixels), $k$ ground truth concepts $\mathbf{c}\in  \mathcal{C} \subseteq \{0, 1\}^k$ (i.e., a binary vector with concept annotations, when available) and $t$ task labels $\mathbf{y} \in  \mathcal{Y} \subseteq \{0, 1\}^t$ (e.g., an image's classes).
During training, a concept-based model is encouraged to align its predictions to task labels i.e., $\mathbf{y} \approx \mathbf{\hat{y}}=f(g(\mathbf{x}))$. Similarly, a concept encoder can be supervised when concept labels are available i.e., $\mathbf{c} \approx \mathbf{\hat{c}} = g(\mathbf{x})$. 
When concept labels are not available, unsupervised concept encoders extract concepts by associating concept labels to clusters found in the embeddings of pre-trained models as proposed by~\citet{ghorbani2019towards,magister2021gcexplainer}. We indicate concept and task predictions as $\hat{c}_i=(g(\mathbf{x}))_i$ and $\hat{y}_j=(f(\mathbf{\hat{c}}))_j$ respectively.


% \paragraph{Concept representations}
% Usually, concept-based models represent concepts using their truth degree, that is, $\hat{c}_1,\ldots,\hat{c}_k\in [0,1]$.
% % the concept encoder $g$ learns $k$ different scalar concept representations $\hat{c}_1,\ldots,\hat{c}_k\in [0,1]$, which represent the predicted truth degree of each concept. 
% However, this representation might significantly degrade task accuracy as observed by~\citet{mahinpei2021promises} and~\citet{zarlenga2022concept}. To overcome this issue, concept-based models may represent concepts using concept embeddings $\mathbf{\hat{c}}_i \in \mathbb{R}^m$ alongside their truth degrees $\hat{c}_i \in [0,1]$.\footnote{%For the sake of simplicity, we abuse notation and 
% With an abuse of notation, we use the same symbol for a concept embedding and its corresponding truth degree, with the former in bold to distinguish it.} While this increase task accuracy of concept-based models~\cite{zarlenga2022concept}, it also weaken their interpretability as concept embeddings lack clear semantics.

\begin{remark}
In the following chapters we will omit the dependency on parameters for morphisms as they are all parametric i.e., instead of writing $f: \mathcal{C} \times \mc{P} \rightarrow \mathcal{Y}$ we will simply write $f: \mathcal{C} \rightarrow \mathcal{Y}$.
\end{remark}

% more examples: \url{https://wikious.com/en/Formal_concept_analysis}

\section{Knowledge gaps and aims}
\label{sec:gaps-concept-learning}
We can summarize the ultimate aim of XAI research on concepts as follows: To design trustworthy AI systems able to attain state-of-the-art performance in solving complex tasks while providing human-understandable explanations for their decisions.
To this end, XAI research on concepts focuses on four main research areas: models, representations, metrics, and explanations. Research in concept models aims to improve the architectures of concept-based models and their concept encoders to increase the performance of these models in learning concepts from raw features and in learning the task labels from the learnt concepts. Research in concept representations focuses on devising more efficient data structures to encapsulate the information of learnt concepts preserving their semantics but allowing for concept encoders to incorporate sample-specific information about specific concept instances. Research in concept metrics aims to assess the quality of learnt concepts in terms of preserved semantics and their predictive information for task labels. Finally, research in concept explanations targets the design of signatures and the forms of the explanations provided by concept-based models in order to make them more trustworthy.

However, XAI research on concepts is a relatively young field and current approaches represent only the first steps towards the ultimate goal of the field. In fact, current approaches struggle either to attain state-of-the-art performances in solving complex tasks or to preserve a clean semantics in learnt concept representations. In addition, state-of-the-art concept-based systems either provide simple explanations in non-formal languages (e.g., Concept Activation Vectors~\cite{kim2018interpretability} or Concept Bottleneck Models~\cite{koh2020concept}), which may mislead human observers, or are not differentiable thus impeding a joint training with concept encoders to learn better concepts depending on the task (e.g., decision trees~\cite{breiman1984classification} or Bayesian rule lists~\cite{letham2015interpretable}). 
% Last but not least, differentiable concept-based models currently generate explanations as a by-product of their decision process. 
We can then summarize the main research directions in this field as follows:
\begin{enumerate}
    \item[\textbf{Aim \#1}]--- Generate compound explanations in formal languages with differentiable concept-based models;
    \item[\textbf{Aim \#2}]--- Attain state-of-the-art performance in solving complex tasks while preserving clean concept semantics;
    % \item[\textbf{Aim \#3}]--- Design interpretable concept-based models whose decision process is based on formal explanations.
\end{enumerate}

The following chapters address some of the main knowledge gaps currently arising in different areas of XAI concept research. In particular, Chapter~\ref{chapter:lens} focuses on \textbf{Aim \#1} presenting Logic Explained Networks (LENs), a family of differentiable concept-based models generating compound explanations in the formal language of first-order logic. Chapter~\ref{chapter:cem} focuses on \textbf{Aim \#2} introducing concept embedding representations which allow concept-based models to attain state-of-the-art performance in solving complex tasks while preserving clean concept semantics. While addressing \textbf{Aim \#2}, existing concept-based models are not designed for concept embeddings and are unable to provide formal and semantically meaningful explanations based on this concept representation. To solve this limitation, Chapter~\ref{chapter:DCR} presents the Deep Concept Reasoner (DCR), the first interpretable concept-based model using concept embeddings. In particular DCR represents the first differentiable concept-based model attaining state-of-the-art performance in solving complex tasks while providing human-understandable and formal explanations for its decisions, thus representing a concrete step towards efficient and trustworthy AI systems.


% \subsection{Concept completeness}


% \subsection{Concept alignment}
% The Concept Alignment Score (CAS) aims to measure how much learnt concept representations can be trusted as faithful representations of their ground truth concept labels. Intuitively, CAS generalises concept accuracy by considering the predictions' homogeneity within groups of similar samples. More specifically, CAS applies a clustering algorithm $\kappa$ to find $\rho > 2$ clusters, assigning to each sample $\mathbf{x}^{(j)}$, for each concept $c_i$, a cluster label $\pi_i^{(j)} \in \{1, \cdots, \rho\}$, using the concept representation $\hat{\textbf{c}}_i$. Given $N$ test samples, the homogeneity score $h(\cdot)$~\citep{rosenberg2007v} then computes the conditional entropy $H$ of ground truth labels $C_i = \{c_i^{(j)}\}_{j=1}^{N}$ w.r.t. cluster labels $\Pi_i = \{\pi_i^{(j)}\}_{j=1}^{N}$, i.e., $h = 1$ when $H(C_i,\Pi_i)=0$ and $h = 1 - H(C_i, \Pi_i)/H(C_i)$ otherwise. The higher the homogeneity, the more a learnt concept representation is ``aligned'' with its labels, and can thus be trusted as a faithful representation. CAS averages homogeneity scores over all concepts, providing a normalised score $\text{CAS} \in [0,1]$:
% \begin{equation}
%     \text{CAS}(\mathbf{\hat{c}}_1, \cdots, \mathbf{\hat{c}}_k) \triangleq \frac{1}{N - 2}\sum_{p=2}^N \Bigg(\frac{1}{k} \sum_{i=1}^k h(c_i, \kappa_p(\hat{\textbf{c}}_i)) \Bigg)
%     % \text{CAS}(\mathbf{\hat{c}}_1, \cdots, \mathbf{\hat{c}}_k) := \frac{1}{k(N-2)} \sum_{p=2}^N \sum_{i=1}^k h(c_i, \kappa(\ha  t{\textbf{c}}_i, p)))
% \end{equation}
% % Notice how when the number of clusters $p$ equals the number of samples, CAS and concept accuracy are identical.
% % The concept alignment score is therefore maximal (i.e., $\text{CAS} = 1$) when all clusters contain only data points which are members of a single concept class (i.e., for all samples within a cluster, the label of any concept $i$ is either always $c_i = True$ or always $c_i = False$). 
% To tractably compute CAS in practice, we sum homogeneity scores by varying $p$ across $p \in \{2, 2 + \delta, 2 + 2 \delta, \cdots, N\}$ for some $\delta > 1$ (details in Appendix). Furthermore, we use k-Medoids~\citep{kaufman1990partitioning} for cluster discovery, similarly to~\citet{ghorbani2019interpretation} and~\citet{magister2021gcexplainer}, and use concept logits when computing the CAS for Boolean and Fuzzy CBMs. For Hybrid CBMs, we use $\hat{\mathbf{c}}_i \triangleq [\hat{\mathbf{c}}_{[k:k + \gamma]}, \hat{\mathbf{c}}_{[i:(i + 1)]}]^T$ as the concept representation for $c_i$ (i.e., the extra capacity is a shared embedding across all concepts).
% % Cluster and concept labels are matched by homogeneity score.
% % and use a simple majority class count for labeling a cluster.

% \begin{itemize}
%     \item Accuracy vs. scalability/noise
%     \item Accuracy vs. explainability
%     \item Accuracy vs. interpretability
% \end{itemize}





% A recent leap in XAI research happened in 2018 when \citet{kim2018interpretability} started the field of concept-based XAI. Their argument was:

% \begin{displayquote}
% A key difficulty, however, is that most ML models operate on features, such as pixel values, that do not correspond to high-level concepts that humans easily understand. Furthermore, a model’s internal values (e.g., neural activations) can seem incomprehensible. We can express this difficulty mathematically, viewing the state of an ML model as a vector space $E_m$ spanned by basis vectors $e_m$ which correspond to data such as input features and neural activations. Humans work in a different vector space $E_h$ spanned by implicit vectors $e_h$ corresponding to an unknown set of human-interpretable concepts. [...] To address these challenges, we [...] provide an interpretation of a neural net's internal state in terms of human-friendly concepts.
% \end{displayquote}

% \section{Notation}
% \begin{itemize}
%     \item input space
%     \item output space
%     \item concept space
%     \item ground truth labels
%     \item predictions / representations
%     \item concept encoder
%     \item concept decoder / label predictor
% \end{itemize}

% \section{The Dark Side of Deep Learning}
% Standard intro w/ motivation for XAI: trade-off between accuracy and explainability: 
% \begin{itemize}
%     \item classical ML models are explainable, but they may have poor performances on some data
%     \item DL has high performances, but is less explainable generating a lack of human trust
% \end{itemize}

% Researchers are trying to: make explainable models more accurate and black boxes more interpretable


% \section{Concept Learning: A New Hope}
% A recent leap in XAI research happened in 2018 when \citet{kim2018interpretability} started the field of concept-based XAI. Their argument was:

% \begin{displayquote}
% A key difficulty, however, is that most ML models operate on features, such as pixel values, that do not correspond to high-level concepts that humans easily understand. Furthermore, a model’s internal values (e.g., neural activations) can seem incomprehensible. We can express this difficulty mathematically, viewing the state of an ML model as a vector space $E_m$ spanned by basis vectors $e_m$ which correspond to data such as input features and neural activations. Humans work in a different vector space $E_h$ spanned by implicit vectors $e_h$ corresponding to an unknown set of human-interpretable concepts. [...] To address these challenges, we [...] provide an interpretation of a neural net’s internal state in terms of human-friendly concepts.
% \end{displayquote}

% The motivation for this research line has roots in cognitive sciences (e.g., Representational Theory of the Mind). According to these theories \textbf{concepts are the basic building blocks of human thoughts}~\citep{margolis2007ontology}: following simple rules the human mind can combine finite stocks of basic concepts over and over again to create increasingly complex representations~\citep{margolis2007ontology}. For instance, the mind can combine the basic concepts ``roof'' and ``walls'' to generate the concept ``house''. 

% These concept theories motivated~\citet{kim2018interpretability} opening a research line on concept learning in ML to increase human trust in AI. The objective of this field is to increase human trust by making AI use ``the same building blocks of human thought'' as opposed to other XAI approaches~\citep{kim2018interpretability}. In particular concept-based XAI has the following aims:
% \begin{itemize}
%     \item discover concepts from trained models to explain their decisions in ``human'' terms (post-hoc explainability)
%     \item train models to learn specific concepts and compose them to solve tasks (self-explainability)
% \end{itemize} 
% Compared to other XAI approaches, concept-based XAI has solid foundations in cognitive sciences and a precise formalization in universal algebra.

% % concepts ontology (philosophy):

% % intuitively, what is a concept? formal concept is defined to be a pair (A, B), where A is a set of objects (called the extent) and B is a set of attributes (the intent) such that the extent A consists of all objects that share the attributes in B, and dually the intent B consists of all attributes shared by the objects in A.

% \section{Elements of Logic(s) and (Formal) Concept Theory}
% % \begin{definition}[Signature~\citep{goguen2005concept}]
% % A signature $\Sigma = (S_f, S_r, \text{ar})$ is a collection of:
% % \begin{itemize}
% %     \item a set of \textbf{function symbols} $S_f$
% %     \item a set of \textbf{relation symbols} (or predicates) $S_r$
% %     \item \textbf{a morphism} $\text{ar}: S_f \cup S_r \rightarrow \mathbb{N}$, which assigns a natural number called \textit{arity} to every function or relation symbol
% % \end{itemize}
% % \end{definition}

% Intuitively, what is a concept? It's (mental) representation describing objects sharing some common properties \citep{margolis2007ontology}. In the following we formalize this idea and provide some concrete examples.

% \subsection{Institutions and Abstract Formal Concept}

% Let $\mathbb{C}at^{op}$ represent the opposite of the category of small categories.
% \begin{definition}[Institution~\citep{goguen2005concept}]
% An institution $\mathbb{I}$ consists of:
% \begin{itemize}
%     \item an abstract category $\mathbb{S}ign$ of signatures
%     \item a functor $Sen: \mathbb{S}ign \rightarrow \mathbb{S}et$ giving for each signature $\Sigma$ the set of sentences $Sen(\Sigma)$, and for each signature morphism $\sigma: \Sigma \rightarrow \Sigma'$, the sentence translation map $Sen(\sigma): Sen(\Sigma) \rightarrow Sen(\Sigma')$
%     \item a functor $Mod: \mathbb{S}ign \rightarrow \mathbb{C}at^{op}$ giving for each signature $\Sigma$ the category of models $Mod(\Sigma)$, and for each signature morphism $\sigma: \Sigma \rightarrow \Sigma'$, the reduct functor $Mod(\sigma): Mod(\Sigma') \rightarrow Sen(\Sigma)$
%     \item a satisfaction relation $\models_\Sigma \subseteq |Mod(\Sigma)| \times Sen(\Sigma)$ for each signature $\Sigma \in \mathbb{S}ign$ such that for each signature morphism, the following satisfaction condition holds (``truth under context morphisms''): $M' \models_{\Sigma'} Sen(\sigma) \iff Mod(\sigma)(M') \models_\Sigma \phi$ for each $M' \in Mod(\Sigma')$ and $\phi \in Sen(\Sigma)$.
% \end{itemize}
% \end{definition}

% We say $M \models_\Sigma T$ where T is a set of $\Sigma$-sentences, if $M \models_\Sigma \phi$ for all $\phi \in T$, and we say $T \models_\Sigma \phi$ if for all $\Sigma$-models $M$, $M \models_\Sigma T$ implies $M \models_\Sigma \phi$.

% \begin{corollary}
% First order logic is an institution.
% \end{corollary}

% \begin{displayquote}
% Other logics follow a similar pattern, including modal logics, temporal logics, many sorted logics, equational logics, order sorted logics, description logics, higher order logics, etc. Database systems of various kinds are also institutions, where database states are contexts, queries are sentences, and answers are models.
% \end{displayquote}

% Given an institution $\mathbb{I}$, a theory is a pair $(\Sigma,T)$, where $T$ is a set of $\Sigma$-sentences. The collection of all $\Sigma$-theories can be given a lattice structure, under inclusion: $(\Sigma,T) \leq (\Sigma',T') \iff \Sigma \subseteq \Sigma' \wedge T \subseteq T'$.

% For any signature $\Sigma$ of an institution $I$, there is a Galois connection between its $\Sigma$-theories and its sets of $\Sigma$-models i.e., $(\Sigma,T)^\bullet = \{M \ | \ M \models_\Sigma T\}$, and if $\mathcal{M}$ is a collection of $\Sigma$-models, let $\mathcal{M}^\bullet = \{\phi \ | \ \mathcal{M} \ \models_\Sigma \phi\}$.

% \begin{definition}[(Abstract) Formal Concept~\citep{goguen2005concept}]
% An (abstract) formal concept of an institution $\mathbb{I}$ is a pair $(T,\mathcal{M})$ such that $T^\bullet = \mathcal{M}$ and $\mathcal{M}^\bullet = T$ (i.e., a closed theory).
% \end{definition}

% This is the most general definition of a concept embracing all current concept theories (e.g., John Sowa’s lattice of theories [61] (abbreviated LOT), the formal concept analysis (FCA) of Rudolf Wille [15], the information flow (IF) of Jon Barwise and Jerry Seligman [3], Gilles Fauconnier’s logic-based mental spaces [13], Peter Gardenfors geometry-based conceptual spaces [16], the conceptual integration (CI, also called blending) of Fauconnier and Turner, etc.). However, to make the notion of concept more concrete and directly applicable to standard ML language we will define a ``concrete'' formal concept in the (standard) institution of First Order Logic (FOL) and in the context of ML.


% \subsection{Concrete Formal Concepts and First Order Logic}

% In a more concrete setting, we will consider in the following entities in the institution of First Order Logic (FOL):
% \begin{itemize}
%     \item the function symbol $g$ (also known as \textbf{concept encoder})
%     \item the function symbol $f$ (also known as \textbf{concept decoder} or \textbf{label predictor})
%     \item the set of \textbf{formal objects} $G$ (FOL models)
%     \item the set of \textbf{formal attributes} $M$ (FOL sentences)
%     \item the binary relation between formal objects and attributes $I \subseteq G \times M$
% \end{itemize}
