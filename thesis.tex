%%%%%
%%
%% Sample document ``thesis.tex''
%%
%% Version: v0.2
%% Authors: Jean Martina, Rok Strnisa, Matej Urbas
%% Date: 30/07/2008
%%
%% Copyright (c) 2008-2011, Rok Strni≈°a, Jean Martina, Matej Urbas
%% License: Simplified BSD License
%% License file: ./License
%% Original License URL: http://www.freebsd.org/copyright/freebsd-license.html
%%%%%

% Available documentclass options:
%
%   <all `report` document class options, e.g.: `a5paper`>
%   withindex   - enables the index. New index entries can be added through `\index{my entry}`
%   glossary    - enables the glossary.
%   techreport  - typesets the thesis in the technical report format.
%   firstyr     - formats the document as a first-year report.
%   times       - uses the `Times` font.
%   backrefs    - add back references in the Bibliography section
%
% For more info see `README.md`
% \documentclass[withindex,glossary]{cam-thesis}
\documentclass[withindex,glossary]{cam-thesis}

% Citations using numbers
\usepackage[numbers]{natbib}
\usepackage{bibentry}
\usepackage{url}


\bibliographystyle{plainnat}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Thesis meta-information
%%

%% The title of the thesis:
\title{Interactive Concept-Aware Learning\\or\\Beyond the Accuracy-Explainability Trade-Off}

%% The full name of the author (e.g.: James Smith):
\author{Pietro Barbiero}

%% College affiliation:
\college{Clare College}

%% College shield [optional]:
% \collegeshield{CollegeShields/Christs}
% \collegeshield{CollegeShields/Churchill}
\collegeshield{CollegeShields/Clare}
% \collegeshield{CollegeShields/ClareHall}
% \collegeshield{CollegeShields/CorpusChristi}
% \collegeshield{CollegeShields/Darwin}
% \collegeshield{CollegeShields/Downing}
% \collegeshield{CollegeShields/Emmanuel}
% \collegeshield{CollegeShields/Fitzwilliam}
% \collegeshield{CollegeShields/Girton}
% \collegeshield{CollegeShields/GonCaius}
% \collegeshield{CollegeShields/Homerton}
% \collegeshield{CollegeShields/HughesHall}
% \collegeshield{CollegeShields/Jesus}
% \collegeshield{CollegeShields/Kings}
% \collegeshield{CollegeShields/LucyCavendish}
% \collegeshield{CollegeShields/Magdalene}
% \collegeshield{CollegeShields/MurrayEdwards}
% \collegeshield{CollegeShields/Newnham}
% \collegeshield{CollegeShields/Pembroke}
% \collegeshield{CollegeShields/Peterhouse}
% \collegeshield{CollegeShields/Queens}
% \collegeshield{CollegeShields/Robinson}
% \collegeshield{CollegeShields/Selwyn}
% \collegeshield{CollegeShields/SidneySussex}
% \collegeshield{CollegeShields/StCatharines}
% \collegeshield{CollegeShields/StEdmunds}
% \collegeshield{CollegeShields/StJohns}
% \collegeshield{CollegeShields/Trinity}
% \collegeshield{CollegeShields/TrinityHall}
% \collegeshield{CollegeShields/Wolfson}
% \collegeshield{CollegeShields/CUniNoText}
% \collegeshield{CollegeShields/FitzwilliamRed}

%% Submission date [optional]:
% \submissiondate{November, 2042}

%% You can redefine the submission notice [optional]:
% \submissionnotice{A badass thesis submitted on time for the Degree of PhD}

%% Declaration date:
\date{My Month, My Year}

%% PDF meta-info:
\subjectline{Computer Science}
\keywords{one two three}


% too long, , 1 liner
% P1: open problem, 
% P2: current solutions -> problem remains
% P3: research questions (reporting on completed work)
%   - max 2 questions
%   - key innovations
% LENS: sparse attention mechanism for concepts -> predictions + truth table (logic) -> explainability by design
% CEM: concept embeddings -> break Acc-vs-Expl trade-off
% how do you measure?
% when we don't have labels?
% gaining interpretability without sacrificing accuracy

% remind story at the beginning and end of each chapter
% abstract: 1 page long
% write asbtract first
% write core chapters second
% chapter 2: highlight gaps + concepts ~20% + assume DL
% 
% ask x viva to check: story + what to do with mix pubs

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Abstract:
%%
\abstract{%
% The opaque reasoning of neural networks induces a lack of human trust.
Human trust in deep neural networks is currently an open problem as their decision process is opaque. 
Current solutions addressing this problem either (i) provide post-hoc, qualitative, and local explanations, or (ii) increase models' transparency undermining their accuracy.
To address these limitations, in this work I demonstrated how to design ``explainable-by-design'' neural models providing quantitative and global explanations without sacrificing accuracy. My key innovations enabling this progress were (i) a sparse attention mechanism and (ii) a high-dimensional representation for concepts learnt during training. On one side, the sparse attention mechanism allowed concept bottleneck models to solve each task using a small subset of relevant concepts and to learn simple logic-based explanations at train time. On the other side, the high-dimensional representation of concepts broke the information bottleneck of these concept-based models allowing them to gain accuracy without sacrificing interpretability. To make the proposed approaches more general, I devised an unsupervised concept layer which reduced training costs and allowed to train these models in absence of concept labels. As existing metrics were not always directly applicable, I proposed new performance scores to validate the impact of my innovations and to compare them with the current state of the art. The results of my experiments demonstrated how the proposed approaches significantly outperformed state-of-the-art models in predictive performance while providing accurate global explanations, thus breaking the current accuracy-vs-explainability trade-off and laying the foundations for human trust in deep learning.
}

% current limitations/knowledge gaps in XAI
% - accuracy and explainability
%   - visual (qualitative), local (instance-based), low-level (input-based), and post-hoc explanations
%   - interpretable models make accuracy worse (trade-off)
% - concepts make explanations more stable, but (1) require (expensive) annotations, and (2) do not solve the other issues
% my goals are
% - build models which are (a) more accurate and (b) more trustworthy than current models
% - provide models which (a) are explainable-by-design (i.e., they do not require a post-hoc external XAI method), (b) provide quantitative, global, and concept-based (high-level) explanations humans can easily understand and interact with



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Acknowledgements:
%%
% \acknowledgements{%
%   My acknowledgements ...
% }



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Glossary [optional]:
%%
% \newglossaryentry{HOL}{
%     name=HOL,
%     description={Higher-order logic}
% }



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Contents:
%%
\begin{document}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Title page, abstract, declaration etc.:
%% -    the title page (is automatically omitted in the technical report mode).
\frontmatter{}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Thesis body:
%%
\chapter*{Timetable}
\textit{A timetable that schedules the remaining work and indicates when the draft and final versions of the thesis will be produced.}


\section*{Objectives}
My research objectives for rest of my Ph.D. are:
\begin{itemize}
    \item Extend the differentiable concept discovery technique to all neural architectures.
    \item Limit shortcut learning in concept-based models.
    \item Write the thesis.
\end{itemize}


\section*{Timeline}

\paragraph{Summer 2022 (July-September)}
\begin{itemize}
    \item Work on NeurIPS reviews.
    \item Extend differentiable concept discovery to CNNs and KGEs.
	\item Experiment different solutions to limit shortcut learning.
\end{itemize}
\textbf{Milestone(s): preliminary results on CNNs and KGEs.}

\paragraph{Michaelmas 2022 (October-December)}
\begin{itemize}
	\item Extend differentiable concept discovery to transformers.
	\item Fine-tune shortcut learning solutions.
	\item Draft the thesis.
\end{itemize}
\textbf{Milestone(s): preliminary results on transformers.}

\paragraph{Lent 2023 (January-March)}
\begin{itemize}
    \item Ask feedback on thesis draft.
    \item Extend the experiments on concept discovery and shortcut learning to real-world scenarios.
\end{itemize}
\textbf{Milestone(s): real-world tests.}

\paragraph{Easter 2023 (April-June)}
\begin{itemize}
    \item Refine implementations.
    \item Write up and submit manuscript(s) to appropriate venue.
\end{itemize}
\textbf{Milestone(s): submit manuscript(s).}

\paragraph{Summer 2023 (July-September)}
\begin{itemize}
    \item Improve thesis.
    \item Work on the reviews of submitted manuscript(s).
\end{itemize}
\textbf{Milestone(s): submit thesis final version.}


\textit{A report on progress made in relation to that described in the first-year PhD Proposal. This should include an indication of where the student is relative to their original timetable, discussion of any significant changes to the original ideas and their implications for the research as a whole.}

In my first Ph.D. year I analyzed the explainable AI (XAI) literature looking for the main knowledge gaps. According to my analysis state-of-the-art XAI methods: (i) were limited to local (instance-based) explanations, (ii) provided brittle and qualitative explanations, or (iii) needed to sacrifice prediction performances on challenging tasks. During the first year of Ph.D. I addressed these knowledge gaps improving state-of-the-art concept-based models~\citep{ghorbani2019towards,kim2018interpretability}. In particular, I contibuted to the invention of Logic Explained Networks (LENs)~\citep{ciravegna2021logic,barbiero2021entropy}, explainable-by-design concept-based architectures providing robust first-order logic logic explanations for their predictions. In my first-year Ph.D. Proposal I also suggested a few preliminary research directions to extend and improve explainable-by-design concept-based architectures.

In my second year of Ph.D. I further analyzed the main gaps of explainable-by-design concept-based architectures preventing their deployment on a large scale, following the suggestions and feedback I received for my first-year viva. I identified three main issues: (i) explainable-by-design architectures did not provide any advantage over their black-box equivalents in terms of raw prediction performance, (ii) existing metrics to evaluate the quality of concept representations were unefficient or based on unrealistic assumptions, and (iii) robust concept-based models required expensive concept annotations for training. During my second year I addressed these knowledge gaps by: (i) contributing to the invention of Concept Embedding Models~\citep{zarlenga2022concept}, explainable-by-design architectures outperforming their black-box equivalents in terms of raw prediction accuracy, (ii) proposing two novel metrics~\citep{zarlenga2021quality,zarlenga2022concept} generalizing and relaxing the assumptions of existing scores measuring the quality of concept representations, and (iii) devising a concept encoding strategy to make graph neural networks aware of concepts they discover in an unsupervised way~\citep{magister2022encoding}.

In summary, in my first two years of Ph.D. I demonstrated how the explainable-by-design architectures I contributed to provide robust trust certificates to their users~\citep{shen2022trust}. More specifically the invented models consistently outperform exiting black-box models in terms of raw prediction performance as well as in terms of quality and stability of their explanations. My plan for the last year of Ph.D. is to extend the unsupervised concept encoding strategy to embrace all the most common architectures. This will allow cutting the expenses for the adoption of concept-based models in different domains. Finally, I will conclude my thesis demonstrating the applicability of  the proposed concept-based models in high-stakes scenarios such as modeling medical digital twins.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Introduction:
%%
\chapter{Introduction} \label{chapter:intro}
\textbf{Research: completed. Status: drafted. Difficulty: low. Priority: low.}

\textit{I will start my thesis with an overview of the state of the art of AI in research and companies and the potential impact of this field on humankind in the next decades. Next, I will discuss the current limitations and knowledge gaps which question the practical deployment of AI in high-stakes decision settings. I will conclude this chapter with an overview of my work describing how it advances the field AI and it contributes to a fairer and safer interaction with humankind.}

Deep learning researchers stockpile ground-breaking achievements almost as fast as they find (consistently similar!) flaws in their models~\citep{marcus2022very}. The extremely high learning capacity may allow deep learning to achieve super-human performances on some tasks at the cost of making impossible even for researchers to trace back and explain incorrect predictions. As this trend got worse, lawmakers started questioning the ethical and legal ramifications of their deployment in safety-critical domains. As a response, the research community intensified the effort in developing trustworthy, fair and reliable models. This effort lead to relevant innovations aiming at explaining the inner workings of deep neural networks. However, after years of research, trustworthy deep learning models are still outside our reach.

The key requisite for human trust is for an agent to show consistent and reliable behavior~\citep{shen2022trust}. The assessment of agents' behavior is commonly measured in terms of (i) task performance i.e., the capacity of the agent to provide \textbf{\textit{accurate predictions}} for test samples, and (ii) rationale i.e., the capacity of the agent to give \textbf{\textit{explanations}} for its predictions. While intense efforts lead to consistent advances in terms of explaining trained ``black-box'' models, most of these approaches turned out to be subject of similar limitations: they are mostly qualitative (mostly visual), local (instance-based), low-level (input-based), and post-hoc (they do not make a model trustworthy by design, they try to check if a model can be trusted). A first sign of change came only recently when \citet{koh2020concept} proposed to supervise the last hidden layer of neurons with human annotated concepts. This allowed the network to (i) be aware of ground-truth human concepts at training time, (ii) use learnt concepts to provide more intuitive high-level explanations, and (iii) interact with human experts correcting mispredicted concepts at test time. While this design significantly improved human trust, it did not solve the issue as (i) the explanations were still mostly local and qualitative and (ii) enforcing concept supervisions during training lead to worse task performance. As a result, finding a good compromise between accurate predictions and robust explanations remains one of the fundamental open problems in deep learning.

This work aims at improving the current trade off between accuracy and explainability by proposing novel model designs (i) showing higher task performance compared to the state of the art, and (ii) providing quantitative and global explanations for their predictions. This work opens with an introduction to deep learning (Chapter~\ref{chapter:intro}) and explainability literature with an in-depth discussion on the main limitations of current methods including concept-based and logic-based models, metrics, datasets, and benchmarks (Chapter \ref{chapter:background}). The central chapters describe the main technical advances of this work. Chapter~\ref{chapter:metrics} discusses quantitative metrics that will be used to analyze and compare architectures throughout this work in terms of their task performance and explanation quality. Chapter~\ref{chapter:lens} addresses the ``\textit{explainability problem}'' through Logic Explained Networks i.e., neural networks providing quantitative and global concept-based explanations for their predictions. Chapter~\ref{chapter:cem} addresses the ``\textit{trade off problem}'' through Concept Embedding Models i.e., concept-based models going beyond the current compromise between accuracy and explainability. Chapter~\ref{chapter:unsupervised} extends the methods described in previous chapters to settings where (expensive!) concept annotations are not available, and must then be learnt in an unsupervised way. Chapter~\ref{chapter:applications} showcases real-world case studies and results in diverse settings from vision to biomedical applications. The last chapter provides a summary of the advances, drawing conclusion and future perspectives (Chapter~\ref{chapter:conclusion}).

\bigskip

\textbf{PAPERS}
\nobibliography*
\begin{enumerate}
    \item \bibentry{ciravegna2021logic}
    \item \bibentry{barbiero2021entropy}
    \item \bibentry{jain2022extending}
    \item Federico Siciliano, Pietro Barbiero, ..., Pietro Lio'. Explaining Neural Networks Using a Ruleset Based on Interpretable Concepts. \textit{arXiv preprint arXiv:XXXX.YYYYY},2023
    \item \bibentry{georgiev2022algorithmic}
    \item \bibentry{zarlenga2021quality}
    \item \bibentry{zarlenga2022concept}
    \item Han Xuanyuan, Pietro Barbiero, ..., and Pietro Lio' Analysing the Neurons of Graph Neural Networks: Towards Concept-Based Global Interpretability. \textit{arXiv preprint arXiv:XXXX.YYYYY}, 2023
    \item \bibentry{magister2022encoding}
    \item Pietro Barbiero, and Pietro Lio'. Logic-based Deep Learning Clinical models: Down Syndrome case study. \textit{arXiv preprint arXiv:XXXX.YYYYY}, 2023
    \item \bibentry{kidwai2023forecasts}
    \item \bibentry{barbiero2021graph}
\end{enumerate}

\section{The Good, the Bad, and the Ugly in AI}

\section{Explainable AI to the Rescue}

\section{Limitations and Knowledge Gaps in Explainable AI}

\section{Summary and Contributions}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Related works:
%%
\chapter{Concept Learning Awakens} \label{chapter:background}
\textbf{Research: completed. Status: drafted. Difficulty: low. Priority: low.}

\textit{In the first part of the chapter I will bring the reader in the amazing field of deep learning. I will first provide an overview of the field, give appropriate definitions, introduce the notation, and describe the details of architectures and learning algorithms. I will use this overview as a basis to demonstrate with simple examples why humans do not stand a chance in understanding what deep learning models learn. I will then illustrate the ethical repercussions of the opaqueness of such models which prevent their fair and safe deployment.}

\textit{In the second part of the chapter I will describe the main approaches for explaining deep learning models and for designing more transparent modules~\citep{duran2021afraid,lo2020ethical,wachter2017counterfactual,gdpr2017,rudin2019stop} with specific details on robust and philosophically-/psychologically-grounded methods such as concept-based approachs~\citep{ghorbani2019towards,kim2018interpretability,shen2022trust}. I will then expose the main limitations of explainable AI and the current grand challenges. I will focus with more attention on the weaknesses my work aims at addressing, including: the lack of clear metrics for concept-based approaches, the lack of differentiable methods for mimicking human biases in using concepts for decision making, the accuracy-vs-interpretability trade-off of explainable AI methods, and the need for expensive manual annotations for differentiable concept-based approaches.}

\section{Notation}

\section{The Dark Side of Deep Learning}

\section{Concept Learning: A New Hope}

\section{The Darkness Strikes Back}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Concept Quality:
%%
\chapter{Concept Quality} \label{chapter:metrics}
\textbf{Research: completed. Status: drafted. Difficulty: low. Priority: low.}

\textit{In this chapter I will discuss how to measure the quality of concept representations. In particular I will focus on my contribution in inventing the niche impurity score~\citep{zarlenga2021quality} which generalizes concept completeness~\citep{yeh2020completeness} to concept subsets. I will demonstrate how this metric is computationally efficient and does not require concept labels thus making it applicable in real-world supervised and unsupervised scenarios. I will conclude the chapter with experiments showing how the niche impurity score can be used in practice to evaluate the robustness of concept representations generated by state-of-the-art supervised and unsupervised concept learning methods.}

\section{Rich or Pure? Trade-Offs in Concept Representations}

\section{Concept Impurity Scores}

\section{Comparing Representations in Concept-based Models}

\section{Robustness and Scalability}

\section*{Papers}
\nobibliography*
\begin{itemize}
    \item \bibentry{zarlenga2021quality}
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Concepts for logic explanations:
%%
\chapter{Concept-based Logic Explanations of Neural Networks} \label{chapter:lens}
\textbf{Research: completed. Status: drafted. Difficulty: low. Priority: low.}

\textit{In this chapter I will explain how concepts can be used to train more interpretable models. In particular, I will focus on my contributions in inventing Logic Explained Networks (LENs)~\citep{ciravegna2021logic}, a family of concept-based models providing first-order logic explanations for their predictions. I will describe LENs main architectures, learning paradigms, and logic rule extraction algorithms~\citep{barbiero2021entropy}. I will conclude the chapter showcasing LENs on a set of synthetic and real-world experiments demonstrating how LENs can provide highly accurate explanations with classification performances close to state-of-the-art models.}


Summary to this point: to solve the problem of human trust in AI we need reliable models that can be accurate and provide explanations for their decisions, we want to know how! In this chapter I will explain how to design self-explaining models providing global and quantitative explanations for their predictions.

\section{Motivation}

Knowledge gap/Motivation: current solutions provide post-hoc, local, and qualitative explanations. Post-hoc is bad because the damage is done: whatever biases the model learnt during training can be identified but not changed. Local is bad because local explanations are brittle: explain a cherry-picked example or an outlier does not generalize. Qualitative is bad because it's not clear how general are explanations: would they hold on unseen samples? for how many samples would they hold? how to compare XAI models? Hence we want: self-explainable models providing global and quantitative explanations. Concept bottleneck models tried to solve the post-hoc problem architecturally/by design: they propose an architecture where models' predictions are conditioned on concepts. This way: predictions can be explained in terms of concepts being active/inactive. However, explanations are still qualitative e.g., this concept is important, this one is less important. Is it possible to formalize the explanation in a way that is not ambiguous and shows how the model used the concepts? Also, explanations are still local: for this sample this concept A is active, concept B is inactive, etc. How to provide global explanations?

Contribution: a self-explainable model providing global and quantitative explanations.

Key innovation: a sparse attention mechanism for concept bottleneck models.

Expected outcome: The sparse attention allows the model to learn how to cherry-pick the most relevant concepts for each task and use only them for solving the task. This way the model learns how to solve the task using only a few concepts which can be active/inactive. This allows the extraction of logic explanations as the model is learning a logic mapping from (fuzzy) concepts to (classification) tasks.The logic explanations are simple thanks to the sparse attention mechanism. Without the attention mechanism the extraction of logic rules won't scale as the truth tables may explode (their size grows as exponentially with the number of concepts). Logic explanations can also be evaluated with quantitative metrics in terms of performance/interpretability: 

- performance: prediction accuracy (using the logic formulae to predict unseen samples)

- interpretability: complexity (the number of terms in a logic formula)

Research questions: how would this approach compare with existing approaches? is it interpretable as white boxes in terms of rule complexity? is it accurate as black boxes? what's the accuracy-vs-explainability trade-off of the approach? is there any advantage of using this approach over existing methods?


\section{Logic Explained Networks}

\subsection{Generate Concept-based Logic Explanations}
The XOR problem

\subsection{The Entropy Layer}

\section{Experiments and Results}

\section{Impact and Significance}
Summary of contributions: a sparse attention mechanism for concept bottlenecks.

Summary of results: a scalable, self-explaining neural approach providing first-order logic explanations for its predictions. The new approach is almost as accurate as black box models (e.g., an equivalent neural model with same number of parameters/layers, random forest). The explanations provided by the new approach are: (i) much more concise than those provided by existing white box rule learners (e.g., decision trees and bayesian rule lists), (ii) as accurate as the ones provided by existing white boxes. It's a way to rigorously assess whether concept-based models learn as intended by checking how they arrived to a prediction.
- the concept-based and the XAI field as novel self-explaining approach which may encourage further advances in concept learning

- other research disciplines as the extraction of FOL rules can be useful to answer research questions in a human interpretable way

- production/society as self-explaining models might become a legal requirement


Impact/significance: self-explaining models may soon become a legal requirement in Europe for the ethical deployment of AI models. So, this work has an impact on:

- the concept-based and the XAI field as novel self-explaining approach which may encourage further advances in concept learning

- other research disciplines as the extraction of FOL rules can be useful to answer research questions in a human interpretable way

- production/society as self-explaining models might become a legal requirement


\section{The Accuracy-vs-Interpretability Trade-Off}
Next steps: the proposed approach provides a good compromise between accuracy and explainability and it's Pareto optimal compared to black and white-boxes but it does not outperforms existing approaches on BOTH explainability and accuracy. How can we solve this trade-off?



\section*{Papers}
\nobibliography*
\begin{itemize}
    \item \bibentry{ciravegna2021logic}
    \item \bibentry{barbiero2021entropy}
    \item \bibentry{jain2022extending}
    \item Federico Siciliano, Pietro Barbiero, ..., Pietro Lio'. Explaining Neural Networks Using a Ruleset Based on Interpretable Concepts. \textit{arXiv preprint arXiv:XXXX.YYYYY},2023
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Beyond the Accuracy-vs-Interpretability Trade-Off:
%%
\chapter{Cracking Concept Bottlenecks Trade-Offs} \label{chapter:cem}
\textbf{Research: completed. Status: drafted. Difficulty: low. Priority: low.}

\textit{In this chapter I will illustrate how concepts can be used to train interpretable models outperforming their non-interpretable equivalents in terms of raw task performance. In particular, I will focus on my contributions in inventing Concept Embedding Models (CEMs)~\citep{zarlenga2022concept} which are now the state of the art of supervised concept-based models. I will describe CEMs architectures and learning paradigms. I will also discuss how CEMs support effective human interactions through learnt concepts highlighting how these interactions can increase both model performances and human trust in the model~\citep{shen2022trust}. I will conclude the chapter demonstrating how CEMs outperform equivalent non-interpretable architectures and state-of-the-art concept-based models on synthetic and real-world datasets.}

Summary to this point: so now we know how to train self-explaining models providing global and quantitative explanations for their predictions. However, we lost a bit of accuracy. In this chapter I will show how to design a self-explaining models without sacrificing accuracy (and vice versa).

\section{Motivation}
Knowledge gap/motivation: concept bottleneck models are powerful and engender human trust because they provide concept-based explanations (awhich are aligned with human catergories + interventions demonstreate the "causal" dependency of the predictions on concepts. However, vanilla CBMs (such as standard E-LENs) come with a cost: they increase interpretability (and trust) but undermine model performance. For this reason, hybrid CBMs~\citep{mahinpei2021promises} were invented trying to get the lost performance back by providing a set of extra unsupervised neurons in the concept bottleneck. This way the concept bottleneck has more "freend logic-based as we have seen in the previous chapter) + humans can interact with the model at concept level at test time by fixing learnt concepts and improving models' performances. These increase human trust as: explanations are based on terms dom" and learning capacity which can be used to increase performance. However (2), this approach impairs interpretability and trust as it weakens the structural dependency of tasks from concepts. In fact, concept interventions are not effective on hybrid CBMs! This means that the model was able to learn both tasks and concepts, but the model does not use the concepts to learn the tasks, but rather the information encoded in the extra unsupervised neurons. This way the whole point of concept bottleneck models is lost. Again, it seems like there is a trade-off between accuracy (hybrid CBMs and vanilla end-to-end black-box) and interpretability (vanilla CBMs). Can e design a model which represents the best of the two worlds? Can we gain accuracy in CBMs without sacrificing interpretability?


Contribution: a concept bottleneck model breaking the information bottleneck at concept-level and going beyond the current accuracy-vs-explainability trade-off.

Key innovation: a high-dimensional representation to learn concepts in the bottleneck.

Expected outcome: the high-dimensional representation increases the capacity of the concept bottleneck models at concept level. The increased model capacity allows to encode more information in each concept not just active/inactive, but nuances representing different aspects of the same concepts enriching the concept semantics with subsymbolic information. This can be useful for explainability as it allows to find relevant subclusters which may represent sub-concepts. However, in this approach ALL neurons are supervised in the bottleneck, thus the prediction still depend on concepts being active/inactive. For this reason, concept interventions are applied as in vanilla CBMs. To evaluate these models and compared them to the current SOTA we use:

- performance: model accuracy

- interpretability/trust: concept alignment score, logic explanation accuracy/complexity, test-time concept interventions

Research questions: how would this approach compare with existing approaches? is it interpretable as white boxes in terms of rule complexity? is it accurate as black boxes? what's the accuracy-vs-explainability trade-off of the approach? is there any advantage of using this approach over existing methods?

\section{The Information Bottleneck in Concept Learning}

\section{Concept Embedding Models}

\section{Beyond the Accuracy-vs-Interpretability Trade-Off}

\section{Interacting with High-Dimensional Concepts}

\section{Robustness and Cost Effectiveness}

Summary of contributions: a scalable, self-explaining neural model providing first-order logic explanations for its predictions without sacrificing accuracy!

Summary of results: the new approach is more accurate than equivalent black boxes and comparable with hybrid CBMs. The concept alignment is as good as in vanilla CBMs and is much higher than in hybrid or fuzzy CBMs for challenging tasks. The mutual information input-to-concepts and concepts-to-task is much higher in CEMs than in vanilla CBMs or black boxes. In CEMs the information is not being compressed at concept level, allowing to break the information bottleneck. CEMs are also efficiently support concept interventions increasing human trust as opposed to hybrid CBMs.

Impact/significance: self-explainability is an ethical and (soon) legal requirement for the deployment of AI-based technologies. However, it's often coming at the cost of reducing models' accuracy. This work solves this trade-off for concept-based models, so it has an impact on:

- the concept-based and the XAI field as novel self-explaining approach which may encourage further advances in concept learning

- other research disciplines as CEMs keep working well in real-world conditions where (expensive!) concept-supervisions are scarce + users can trust predictions (model is accurate) and can interact with the model to verify the "causal" relationship between learnt tasks and concepts (or just improve model performance through interventions)

- production/society as self-explaining models might become a legal requirement and high-performance is a must

\section{A Remark on Concept Annotations}
Even though CEM is efficient in real-world conditions where concept supervisions are scarce, it still requires some supervisions at concept level. Such supervisions might be expensive and in some cases (e.g. biology) concepts might be unknown a priori which makes it impossible to train supervised concept bottleneck models. How can we train concept bottleneck models without supervised concepts?


\section*{Papers}
\nobibliography*
\begin{itemize}
    \item \bibentry{zarlenga2022concept}
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Robust Concept Discovery:
%%
\chapter{Concept Self-Awareness} \label{chapter:unsupervised}
\textbf{Research: in progress. Status: drafted. Difficulty: medium. Priority: high.}

\textit{In this chapter I will illustrate how to make models ``aware'' of concepts they discover during training eliminating the need for expensive (and manual) concept annotations. I will first describe how to make networks learn concept encodings without using any concept label during training. Next I will show how to avoid \textit{shortcut learning}~\citep{geirhos2020shortcut} which can prevent the network to learn a ``complete'' and robust concept representation. I have already obtained preliminary results on graph neural networks~\citep{magister2022encoding} which I submitted to the Neural Information and Processing Systems conference. In the next few months I will focus on extending the approach to other common architectures such as convolutional networks. I will conclude the chapter showing the results on real-world settings comparing supervised and unsupervised concept learning methods.}


Summary to this point: up to here we demonstrated how to design self-explainable models which are as accurate as black boxes (or more) without sacrificing interpretability and the effectiveness of causual human interactions. In this chapter I will show a trick to train concept bottleneck modes for GNNs without expensive and sometimes unknown concept annotations.

\section{Motivation}
Knowledge gap/motivation: CEMs and LENs are robust self-explaining models going beyond the current accuracy-vs-explainability trade-off. However, these models require concept supervisions which might be expensive to generate to train the model, but in some cases might not even be known a priori, which makes impossible to train a concept bottleneck in a supervised way. In this setting there are papers (ACE, GCExplainer) showing how latent concepts can be extracted from trained architectures post-hoc with the assumption that: 1 concept == 1 cluster. However, the existing concept-based unsupervised approaches are post-hoc, while we argue against this family of approaches because they do not increase human trust in the model itself allowing for interventions at test time, nor they try to make the model itself more explainable. So, here we try to find a way to exploit the natural clustering performed in hidden layers of a neural network to make the NN aware of concepts and use them to predict the classification targets, making the architecture self-explainable.

Contribution: a self-explainable GNN model distilling unsupervised concepts at train time and using these concepts to solve the task.

Key innovation: a concept distillation layer based on hard cluster encodings.

Expected outcome: the concept distillation layer generates hard cluster encodings (labels) in the hidden layers of a GNN. Each cluster can be thought as a concept representing a spacific motif/subgraph. A LENs model is trained on top of cluster encodings to generate logic explanations for the predictions of the GNN based on the concepts learnt without concept annotations. The model is evaluated in terms of:

- performance: model accuracy

- interpretability: concept purity (edit distance of motifs in each cluster), concept completeness, logic explanations' accuracy and complexity

Research questions: how would this approach compare with existing approaches (post-hoc such as black-box GNN + GCExplainer)? is it interpretable as white boxes in terms of rule complexity? is it accurate as black boxes? what's the accuracy-vs-explainability trade-off of the approach? is there any advantage of using this approach over existing methods?


\section{The Costs of Supervised (Concept) Learning}

\section{Fantastic Concepts and Where to Find Them}

\section{Encoding Concepts in Neural Networks}

\section{Removing Shortcuts to Concepts}


Summary of contributions: a self-explaining GNN which explains its own predictions based on concepts learnt automatically in an unsupervised way.

Summary of results: the model accuracy of the self-explaining GNN is comparable to a standard GNN. The concept purity is as high as in post-hoc methods such as GCExplainer. Motifs/subgraphs are coherent within each cluster. The learnt set of concepts is complete w.r.t. the task i.e., it is possible to accurately predict the task given the information in learnt concepts. The logic explanation accuracy is just a bit lower than model accuracy (as expected, because that's a boolean mapping). The complexity is really low ~3-4 terms which makes the explanations simple and quickly interpretable.

Impact/significance: this work alleviates the problem of generating expensive concept annotations which might even be unfeasible in some fields where humans have not yet accumulated enough knowledge to allow a robust a large-scale concept annotations. So, this work has an impact on:

- the concept-based and the XAI field allowing the expansion of these approaches in tackling problems where concept annotations are too scarce/expensive or not existing.

- other research disciplines especially those where learning new concepts can help humans accumulate knowledge and build an ontology

\section{Architectural biases}
Next steps: scale to non-GNN models which might be tricky because in GNNs clustering works and has a strong association to motifs, while other architectures do not have such a strong bias.

\section*{Papers}
\nobibliography*
\begin{itemize}
    \item Steve Azzolin, Pietro Barbiero, ..., and Pietro Lio' Global GNN Interpretability via Logic Explanations. \textit{arXiv preprint arXiv:XXXX.YYYYY}, 2023
    \item Han Xuanyuan, Pietro Barbiero, ..., and Pietro Lio' Analysing the Neurons of Graph Neural Networks: Towards Concept-Based Global Interpretability. \textit{arXiv preprint arXiv:XXXX.YYYYY}, 2023
    \item \bibentry{magister2022encoding}
\end{itemize}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Applications to Medical Digital Twins:
%%
\chapter{Applications} \label{chapter:applications}
\textbf{Research: in progress. Status: drafted. Difficulty: medium. Priority: medium.}

\textit{In this chapter I will showcase how my inventions significantly improved medical digital twin models~\citep{laubenbacher2021using}. First, I will descibe existing digital twin approaches in medicine and the main challenges of the field. Next, I will demonstrate how concept-based neural models can significantly improve the flexibility and robustness of existing equation-based approaches. In particular I will show how concept-based models allow the discovery of multi-omic patterns explaining drug responses in asthma and down syndrome.}

\section{Concept Learning for Biomedical Data}

\section{mRNA Expression Profiles in Asthma}

\section{Mouse Models of Down Syndrome}


\section*{Papers}
\nobibliography*
\begin{itemize}
    \item Pietro Barbiero, and Pietro Lio'. Logic-based Deep Learning Clinical models: Down Syndrome case study. \textit{arXiv preprint arXiv:XXXX.YYYYY}, 2023
    \item \bibentry{kidwai2023forecasts}
    \item \bibentry{barbiero2021graph}
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Conclusion:
%%
\chapter{Conclusion} \label{chapter:conclusion}
\textbf{Research: completed. Status: drafted. Difficulty: low. Priority: low.}

\textit{I will conclude my thesis with a summary of my inventions. I will then discuss the impact of my contributions to the deep learning field as well as to broader research communities.}

\section{Summary of the Contributions}

\section{Potential Impact on Research and Society}

\section{The Next Decades in AI}



\chapter*{Other papers}
\nobibliography*

\bibentry{barbiero2020modeling}

\bibentry{barbiero2020computational}

\bibentry{georgiev2021algorithmic}

\bibentry{barbiero2021predictable}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% References:
%%
% If you include some work not referenced in the main text (e.g. using \nocite{}), consider changing "References" to "Bibliography".
%

% \renewcommand to change default "Bibliography" to "References"
\renewcommand{\bibname}{References}
\cleardoublepage
\phantomsection
\addcontentsline{toc}{chapter}{References}
%\bibliographystyle{plainnat}
\bibliography{thesis.bib}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Appendix:
%%

% \appendix



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% The ``PyTorch, Explain!'' Library:
%%
% \chapter{The ``PyTorch, Explain!'' Library}
% \textbf{Research: completed. Status: drafted. Difficulty: low. Priority: low.}

% \textit{In this chapter I will illustrate the ``PyTorch, Explain!'' library, a free and open source python package which I developed and maintained during the PhD. The library contains all the advances I contributed to, including neural modules, metrics, dataset, and experiments. In this chapter I will explain the structure of the library and how to use it from installing the package to reproducing the experiments, from using existing modules in different scenarios to contributing to the codebase.}

% \section{Structure and Documentation}

% \section{Setup}

% \section{Modules}

% \section{Metrics}


% \section*{Papers}
% \nobibliography*
% \bibentry{barbiero2021pytorch}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Index:
%%
\printthesisindex

\end{document}
